---
title: Inseason analysis of chum salmon *Oncorhynchus* *keta* bycatch from shoreside
  sector of Bering Sea Aleutian Islands walleye pollock *Gadus* *chalcogrammus* trawl
  fishery.
author: Patrick Barry^[1],
  Jamie Musbach, Abby XXX, and Wes Larson.
date: "`r paste('Report generated on: ', Sys.Date(),sep='')`"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
    includes:
            in_header: ./latex_fmt/header.tex 
  html_document:
    toc: yes
    toc_depth: 2
bibliography: ./Bib/NOAA_GSI.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)

CrntSW <- 28 #Current Stat week
CrntYr <- format(Sys.Date(),"%Y") #Get the current year

dir.create(file.path(paste("Output/",CrntYr,"/",CrntSW,sep="")))

library(tidyverse)

#functions
statweek = function(dates, format="%d-%m-%Y", ...) {
  # convert to Date
  dates = as.Date(dates, format=format, ...) 
  # get correction for the first week of the year (0 if 1-Jan not a Sunday)
  firstweek = 1 - as.numeric(format(as.Date(cut(dates, "year")), "%U")) 
  output = as.numeric(format(dates, "%U")) + firstweek
  return(output)
}

#Dummy table function
dt = function(label, caption=NULL) {
  print(xtable::xtable(setNames(data.frame(x=numeric()), " "),
               caption=caption,
               label=paste0("tab:", label)), 
        hline.after=NULL,
        booktabs=FALSE,
        size="\\fontsize{0.1pt}{0.1pt}\\selectfont")
}
```

---
subtitle: '`r paste("Results from Statistical Week", CrntSW, sep="")`'
---

\newpage


# Summary
```{r exec-summary, echo = FALSE, ref.label = c("readAKRO", "AKROsummary","BBSRIsampling","GTdat","NameFunkiness","SetupRuns","BSmodelClean")}
```

- Number of chum salmon taken in fishery
- Number of chum salmon sampled (sample rate) & genotyped
- Largerst proportion of fish? - Any trend over the last few weeks?
- Estimate of number of Western Alaska / Upper Middle Yukon Fish

```{r SWresSummary, echo = FALSE, message=FALSE,out.height='3in',out.width='4.5in',fig.align = 'center'}
knitr::include_graphics(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","SW",CrntSW,"_TableAltVrs.png",sep="")))
```

\newpage

# Introduction
```{r readAKRO, eval=T, echo=F}
AKRO_Files <- list.files(file.path(paste("./MainData/AKRO/",CrntYr,sep="")),
                         include.dirs = FALSE) #list AKRO files sent by Glenn

#Find the file to process
FileWeeks <- AKRO_Files %>%
  str_split(.,"_")%>%
  lapply(.,"[[",1)%>%
  unlist() %>% 
  as.Date(.,format="%Y%m%d")%>%
  format(.,"%d-%m-%Y")%>%
  statweek(.)

File2Read <- AKRO_Files[which.min(abs(CrntSW-FileWeeks)) ]

AKRO_Obs <- readxl::read_xls(file.path(paste("./MainData/AKRO/",CrntYr,"/",File2Read,sep=""))) %>%
  mutate(DATE = format(DELIVERY_END_DATE,"%d-%m-%Y"))%>%
  mutate(SW = statweek(DATE))

AKRO_ObsSum <- AKRO_Obs %>%
  group_by(SW,NMFS_AREA) %>%
  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
            nDeliveries = n(),
            nUniqVess = length(unique(DELIVERING_VESSEL_PERMIT)))%>%
  mutate(nChumPerDeliv = nChum/nDeliveries)%>%
    filter(nUniqVess >=3)

```

Within the Federally Managed midwater trawl fishery for walleye 
pollock (*Gadus* *chalcogrammus*) in the Bering Sea and Aleutian 
Islands (BSAI), chum salmon (*Oncorhynchus* *keta*) incidental catch occurs occasionally in very 
large numbers. Salmon are managed as a prohibited species catch (bycatch) 
and are highly regulated. Currently, 
annual estimates of genetic stock composition are produced by NOAA's 
Alaska Fishery Science Center (AFSC) genetics program and presented to 
the North Pacific Fisheries Management Council (NPFMC). All chum salmon bycatch
is enumerated by the observer program and 1 in 30 are sampled; information 
taken on length, weight, sex, and a tissue sample and scale sent to the AFSC 
genetics program for analysis. In 2024 a project was initiated by Bristol Bay
Salmon Research Institute (BBSRI) to sample the bycatch from the shore based 
sector of the fleet in order to obtain weekly estimates of genetic stock 
composition. This report outlines the results the analysis of the chum
salmon bycatch from statistical week `r CrntSW` (`r AKRO_Obs%>% filter(SW == CrntSW) %>% slice_min(order_by=DATE)%>% slice_head(n=1) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b")` - 
`r AKRO_Obs%>% filter(SW == min(AKRO_ObsSum$SW)) %>% slice_max(order_by=DATE)%>% slice_head(n=1) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b")`).

# Methods
## Bycatch Sampling
Port sampler, employed by BBSRI, sampled pollock hauls delivered to processing
plants in Dutch Harbor and Akutan. These samplers are instructed to sample
at the same rate within a statistical week, but variable among weeks 
(changes with the volume of bycatch). The target sampling rate for statistical
week `r CrntSW` was 1 in `r unique(as.numeric(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE))[!is.na(unique(as.numeric(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE)))]`
Port samplers, allowed access to chum salmon bycatch after NMFS observers 
have processed the offload, take a length measurement, scale sample for 
age estimation, and a fin clip for genetic analyses. Fin clips are stapled 
onto a Whatman card which is labelled with haul level information from the 
delivery. Scale analyses will be conducted post-season.

## Genotyping
All tissue samples, from both Dutch Harbor and Akutan, were sent to the
BBSRI staffed genetics laboratory in Dutch Harbor 
for processing. Genomic DNA was extracted from dried finclips 
with Macherey-Nagel (Allentown, PA) NucleoSpin Tissue
kits. Extracted DNA was amplified for 96 single nucleotide polymorphism markers (SNPs)
with a Fluidigm (San Francisco, CA) 
BioMark X9 system with 
96.96 Dynamic Array integrated fluidic circuit (IFC). Each of the 9,216 parallel reactions were 
consisted of
50–500 $\eta g / \mu l$l DNA,
1X Fast GT Sample Loading Reagent (Fluidigm), 
1X TaqMan® GTXpress Master Mix (Applied Biosystems), 
10X Custom ABI TaqMan SNP Genotyping Assay (Applied Biosystems), 
1X Assay Loading Reagent (Fluidigm), and 
2.5X ROX Reference Dye (Invitrogen). The temperature profile for amplification was
thermal mixing at 60°C for 10 min and 70°C for 30 min followed by
“Hot-Start” denaturation at 95°C for 2 min and 40 cycles of 
amplification (denaturation at 95°C for 2 s and annealing at 60°C for 20 s). 
After amplification, genotypes were scored with 
BioMark Genotyping Analysis software.

## Genetic Stock Identification
Mixtures were created by grouping sampled fish into 
temporal groups (statistical week) from non-debriefed observer data 
provided by the Alaska Regional office and linked to genetic samples
by BBSRI. Individual samples were fewer than 80% of their multilocus 
genotype scored were dropped from analyses. Genetic stock identification 
was performed with the conditional 
genetic stock identification model in the R package *rubias*
[@MoranAnderson2019] following the methods used in NOAA's annual reports.
Briefly, baseline populations were
grouped into six regions following [@Gray2010]: Southeast Asia (SE Asia), 
Northeast Asia (NE Asia), Western Alaska
(W Alaska), Upper/Middle Yukon (Up/Mid Yukon), Southwest 
Alaska (SW Alaska), and the Eastern GOA/Pacific Northwest (EGOA/PNW).
For all estimates, the Dirichlet prior parameters for the 
stock proportions were defined by region to be
1/($GC_g$), where $C_g$ is the number of baseline populations 
in region $g$, and $G$ is the number of regions.
To ensure convergence to the posterior 
distribution, six separate MCMC chains of 100,000 
iterations (burn-in of 50,000) of the non-bootstrapped model were 
run, which each chain starting at disparate values of stock 
proportions; configured such that for each chain 95% of the 
mixture came from a single designated reporting group (with 
probability equally distributed among the populations within 
that reporting group) and the remaining 5% equally distributed 
among remaining reporting groups.The convergence of chains 
for each reporting group estimate was assessed with the 
Gelman-Rubin statistic [@GelmanRubin1992] estimated with 
the gelman.diag function in the coda library [@Plummer2006] 
within R. Once chain convergence was confirmed, inference 
was conducted with the conditional genetic stock identification 
model with bootstrapping over reporting groups (MCMC chains of 100,000 
iterations, burn-in of 50,000, 100 bootstrap iterations). 

The stock composition estimates were summarized by the mean, 
standard deviation,
median, 95% credible interval (2.5th and 97.5th percentile of the MCMC iterates in the posterior
output), and $P = 0$, which is the probability that a stock composition estimate is effectively zero
(Munro et al. 2012). The $P = 0$ statistic is the frequency of the last half of the MCMC iterates of
each chain for which the individual regional contribution to the mixture was less than a threshold
of $0.5E^{-6}$. This statistic may be more useful than the credible interval for assessing the presence
or absence of minor stocks. 

# Results 
## Chum Salmon Bycatch  

```{r AKROsummary, eval=T, echo=F}

AKRO_ObsSum <- bind_rows(AKRO_ObsSum %>% mutate(NMFS_AREA = as.character(NMFS_AREA)),
                         AKRO_Obs %>%
                          group_by(SW) %>%
                          summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
                                    nDeliveries = n(),
                                    nUniqVess = length(unique(DELIVERING_VESSEL_PERMIT)))%>%
                          mutate(nChumPerDeliv = nChum/nDeliveries)%>%
                           mutate(NMFS_AREA = "BSAI Total")%>%
                            filter(nUniqVess >3))
                         
  
```

Since statistical week `r min(AKRO_Obs$SW)` (`r AKRO_Obs%>% filter(SW == min(AKRO_ObsSum$SW)) %>% slice_min(order_by=DATE) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b")`),  `r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% ungroup() %>% select(nChum) %>% sum(.,na.rm=T) %>% formatC(.,big.mark = ",")` chum salmon have been incidentally
caught by the shoreside sector. In the current statistical 
week (`r CrntSW`), there were 
`r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum() %>% formatC(.,big.mark = ",")` chum salmon caught in `r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nDeliveries) %>% sum() %>% formatC(.,big.mark = ",")  ` deliveries, `r ifelse((AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) < (AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==(CrntSW-1))%>%ungroup() %>% select(nChum) %>% sum()),"a decrease in total bycatch","an increase in total bycatch")` from the previous week (Figure 1). The majority of chum salmon were caught in NMFS Area `r AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA)%>% unlist()`. `r ifelse(AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA) == AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),paste("Similarly, the largest chum rate (chum per delivery) was also in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),sep=""),paste("Despite the largest number of chum salmon being caught in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA)," the highest rate of chum salmon bycatch was in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),sep=""))`.

```{r AKRO_Bycatch, eval=T, echo=F, fig.cap="Number of chum salmon caught by the shoreside sector of the Bering Sea pollock trawl fishery by statistical week. Weekly totals for which fewer than 4 vessels made deliveries have been omitted.",fig.width=6,fig.height=3}
AKRO_ObsSum %>%
  ggplot(.,aes(x=SW,y=nChum,color=as.factor(NMFS_AREA),group=NMFS_AREA))+
  geom_point()+
  geom_line()+
  theme_bw()+
  ylab("Number of Chum Salmon")+
  xlab("Statistical Week")+
  guides(color=guide_legend(title="NMFS\nStat Area"))+
  scale_y_continuous(labels=scales::comma)

```

## Sampling of Bycatch

```{r BBSRIsampling, eval=T, echo=F}
BBSRI_Dlvry <- readxl::read_xlsx(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/","Catch Sampling_Master_SW",CrntSW,".xlsx",sep="")),sheet = 'DELIVERYINFO') %>%
  mutate(DATE = as.Date(as.character(DATE),"%Y-%m-%d"))%>%
  mutate(DATE2 = format(DATE,"%d-%m-%Y"))%>%
  mutate(SW = statweek(DATE2))%>%
  rename('PROCESSOR_PERMIT' = 'PROCESSOR',
         'OFFLOAD_NUMBER' = 'OFFLOAD_NO')

#Bind to obs data
AKRO_BBSRI_Dlvry <- AKRO_Obs %>%
  full_join(.,BBSRI_Dlvry, by = c('CRUISE','PROCESSOR_PERMIT',"OFFLOAD_NUMBER","SW"))%>% #don't merge by NMFS area, use NMFS_AREA.x from AKRO data in subsequent code
  filter(SW == CrntSW)

if(any(is.na(AKRO_BBSRI_Dlvry$CRUISE) & (AKRO_BBSRI_Dlvry %>% filter(is.na(CRUISE)) %>% select(CHUM_LANDED) %>% sum(.,na.rm=T) %>% {.>0}))==TRUE) {
  "WARNING: Positive chum catches for cruises not in AKRO data"
}else{ AKRO_BBSRI_Dlvry <- AKRO_BBSRI_Dlvry %>% filter(!is.na(CRUISE))
}

```

In statistical week `r CrntSW`, `r AKRO_BBSRI_Dlvry %>% select(SAMPLER_INITIALS) %>% unlist()%>% str_split(.,",|/|\\s") %>% unlist() %>% tolower() %>%unique() %>% {.[!is.na(.)]} %>% {.[.!=""]} %>% length()` BBSRI 
technicians sampled hauls delivered to Dutch Harbor and Akutan. There were `r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum() %>% formatC(.,big.mark = ",")` chum salmon caught in 
statistical week `r CrntSW`. A total of `r AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T)`
were sampled for an overall sampling rate of 
`r (AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% round(.,2)` or 
~`r as.character((AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% janitor::round_to_fraction(.,denominator = 30) %>% MASS::fractions(.))` which 
is `r ifelse((AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum())  < (1/min(as.numeric(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE),na.rm=T)), "less than", "greater than")` the target rate of `r as.character(1/min(as.numeric(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE),na.rm=T) %>% janitor::round_to_fraction(.,denominator = 30) %>% MASS::fractions(.))` (`r (1/min(as.numeric(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE),na.rm=T) %>% janitor::round_to_fraction(.,denominator = 30) %>% MASS::fractions(.))`; Table 1). `r ifelse(AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE) & CHUM_RETENTION_COUNT > 0) %>% select(CHUM_RETENTION_COUNT) %>% sum(.,na.rm=T) %>% {.>0},paste("There were ", AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE) & CHUM_RETENTION_COUNT > 0) %>% select(CHUM_RETENTION_COUNT) %>% sum(.,na.rm=T), " chum salmon unsampled from ", AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE)  & CHUM_RETENTION_COUNT > 0) %>% nrow(.), " deliveries",sep=""),"All deliveries with non-zero chum bycatch were sampled")` (Table 1).   

```{r, SampTable, eval=T, echo=F}
options(knitr.kable.NA = '')

AKRO_BBSRI_Dlvry %>%
  group_by(PLANT_NAME,SW,TARGET_SAMPLE_RATE) %>%
  mutate(TARGET_SAMPLE_RATE = round(1/TARGET_SAMPLE_RATE,3))%>%
  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
            nChumSamp = sum(CHUM_SAMPLED,na.rm=T),
            RlzdSampRt = round(nChumSamp / nChum,3))%>%
bind_rows(.,data.frame(PLANT_NAME = "Total",nChum = sum(AKRO_BBSRI_Dlvry$CHUM_RETENTION_COUNT,na.rm=T),nChumSamp = sum(AKRO_BBSRI_Dlvry$CHUM_SAMPLED,na.rm=T),RlzdSampRt = round(sum(AKRO_BBSRI_Dlvry$CHUM_SAMPLED,na.rm=T)/sum(AKRO_BBSRI_Dlvry$CHUM_RETENTION_COUNT,na.rm=T),2)))%>%
  filter(nChum>0)%>%ungroup()%>%
  select(-SW) %>%
  knitr::kable(col.names=c("Plant","Target Sample\nRate","Total\nChum","Chum\nSampled","Sample\nRate"), booktabs = T, caption = paste("Sampling information for statistical week " , CrntSW, " over the shoreside processing plants.",sep=""))%>%
 kableExtra::row_spec(nrow(AKRO_BBSRI_Dlvry %>%
                  group_by(PLANT_NAME,SW,TARGET_SAMPLE_RATE) %>%
                  mutate(TARGET_SAMPLE_RATE = round(1/TARGET_SAMPLE_RATE,3))%>%
                  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
                            nChumSamp = sum(CHUM_SAMPLED,na.rm=T),
                            RlzdSampRt = round(nChumSamp / nChum,3)))-1,hline_after=T)
```


## Genotyping 

```{r GTdat, eval=T, echo=F}
GenoFiles <- list.files(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/",sep="")))%>%
  grep(pattern='Chip|Run',.,value=T)

GenoMetaDat <- readxl::read_xlsx(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/","Catch Sampling_Master_SW28.xlsx",sep="")),
                                sheet = "BIOSAMPLING") 

Genos_Fldm<-lapply(1:length(GenoFiles),function(x) read_delim(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/",GenoFiles[x],sep="")),delim=",",skip = 16,col_names = F) %>%
         rename('Chamber'= 1 ,
                'Locus' = 2,
                'Allele_x' = 3,
                'Allele_y' = 4,
                'Sample_Name' = 5,
                'Sample_Type' = 6,
                'CallInfo_Auto' = 7,
                'Call_Confidence' = 8,
                'CallInfo_Final' = 9,
                'CallInfo_Converted' = 10,
                'Intensity_x' = 11,
                'Intensity_y' = 12) %>%
           mutate(Chip = str_split(string = GenoFiles[x],pattern="_|Chip") %>% lapply(.,"[[",3)%>% str_trim()%>% unlist()))%>%
  bind_rows(.)%>%
  select(1:12,Chip)

Genos_Fldm_Genotype <- Genos_Fldm %>%
  mutate(CallInfo_Converted = recode(CallInfo_Converted,
                                     'No Call' = "NA:NA",
                                     'NTC' = "NTC:NTC"))%>%
  select(Sample_Name,Chip, Locus,CallInfo_Converted) %>%
  reshape2::dcast(Sample_Name + Chip ~ Locus , value.var= c("CallInfo_Converted"))

rubiasCols <- data.frame(sample_type = "mixture",
                         repunit = NA,
                         collection = "SW28",
                         indiv = Genos_Fldm_Genotype$Sample_Name)

Genos_Fldm_Alleles <- lapply(3:ncol(Genos_Fldm_Genotype),function(x)
  str_split(Genos_Fldm_Genotype[,x],pattern=":") %>%
    {cbind(lapply(.,"[[",1) %>% unlist(),lapply(.,"[[",2) %>% unlist())}
  )%>%
  do.call(cbind,.)%>%
  {cbind(rubiasCols,.)}

colnames(Genos_Fldm_Alleles)[5:(ncol(Genos_Fldm_Alleles))]<-paste(rep(colnames(Genos_Fldm_Genotype)[-c(1:2)],each=2),c("1","2"),sep="_")

```

```{r QC_input, eval=T, echo=F}
#check for samples not genotyped
if(length(GenoMetaDat$`WHAT_CARD-NO`[!GenoMetaDat$`WHAT_CARD-NO`%in%Genos_Fldm_Alleles$indiv])>0){
  paste("WARNING: Samples in biosampling sheet not associated with genotypes (",paste(GenoMetaDat$`WHAT_CARD-NO`[!GenoMetaDat$`WHAT_CARD-NO`%in%Genos_Fldm_Alleles$indiv],collapse=" , "),").",sep="")
} 

#check for samples genotyped multiple times
if(any(duplicated(Genos_Fldm_Alleles$indiv %>% {.[.!="NTC"]}))){
  paste("WARNING: Duplicated samples in genotype file (",paste(Genos_Fldm_Alleles$indiv[duplicated(Genos_Fldm_Alleles$indiv,fromLast = T)] %>% {.[.!="NTC"]},collapse=","),").",sep="")
  }   

nDups<- sum(duplicated(Genos_Fldm_Alleles$indiv %>% {.[.!="NTC"]}))

```

```{r NameFunkiness, eval=T, echo=F}

Baseline <- read.csv("./MainData/ChumBaseline_ABL84_382pops_42071inds_KotzSnd.csv")

Mixtures <- Genos_Fldm_Alleles

Mixtures[,(2:ncol(Mixtures))]<-lapply(((2:ncol(Mixtures))), function(x) as.character(Mixtures[,x]))

Baseline[,2:ncol(Baseline)]<-lapply(2:ncol(Baseline), function(x) as.character(Baseline[,x]))

Mixtures[Mixtures==0]<-NA

# Confirm that the loci names match  
BaseLoci <- colnames(Baseline)[-(1:4)] %>%
  gsub(pattern="\\.1$|_1$|_2$",replacement="",.) %>%
  gsub(pattern="\\.",replacement="_",.) %>%
  tolower()

 colnames(Baseline)[-(1:4)] <- paste(BaseLoci,c("",".1"),sep="")

MixLoci <- colnames(Mixtures)[-(1:4)] %>%
  gsub(pattern="_1$|_1.1$|_2$",replacement="",.) %>%
  gsub(pattern="-",replacement="_",.) %>%
  tolower()
  
colnames(Mixtures)[-(1:4)] <- paste(MixLoci,c("",".1"),sep="")

Loci2RM <- MixLoci[!MixLoci %in% BaseLoci][c(T,F)]

Mixtures <- Mixtures %>%
  select(-(contains(Loci2RM)))

BaseLoci <- colnames(Baseline)[-(1:4)]
MixLoci <- colnames(Mixtures)[-(1:4)]

if(any(BaseLoci != MixLoci)){ 
  LociOrder <- lapply(BaseLoci[c(T,F)],function(BL) which(MixLoci %in% BL)) %>%
    unlist() +4
} else {
    LociOrder <- 5:(84*2+4)
  }

Mixtures<-Mixtures[,c(1:4,LociOrder)]

if(any(colnames(Baseline)!= colnames(Mixtures))){
  "WARNING!!!! rubias baseline and mixture files have different locus order."
}

#remove NTC sample
Mixtures <- Mixtures %>% 
  filter(indiv != "NTC")

#Samples with low genotyping success?
nloci<- (ncol(Mixtures)-4) / 2

BadInds <- Mixtures %>%
  mutate(CntNA = rowSums(is.na(select(., starts_with("oke")))),
          GTrate = round(1-(CntNA / (nloci*2)),2))%>%
  filter(GTrate<0.8)

if(nrow(BadInds)>0){
  Mixtures<- Mixtures %>%
    filter(!(indiv%in%BadInds$indiv))
}

# close_matching_samples(Mixtures,
#                               gen_start_col = 5)

```

Of the `r AKRO_BBSRI_Dlvry %>% filter(SW == CrntSW) %>% select(CHUM_SAMPLED) %>% sum(.,na.rm=T)` chum salmon sampled
from the total catch of `r AKRO_Obs %>% filter(SW == CrntSW) %>% select(CHUM_RETENTION_COUNT) %>% sum(.,na.rm=T)`, 
`r GenoMetaDat%>%nrow()` (`r GenoMetaDat%>%nrow()/AKRO_BBSRI_Dlvry %>% filter(SW == CrntSW) %>% select(CHUM_SAMPLED) %>% sum(.,na.rm=T) *100`%) were extracted and amplified for the SNP marker panel. Of those 
that were amplified, `r ((GenoMetaDat%>%nrow() -nrow(BadInds) - nDups)/AKRO_BBSRI_Dlvry %>% filter(SW == CrntSW) %>% select(CHUM_SAMPLED) %>% sum(.,na.rm=T))*100 %>% round(.,3)`% passed quality control filters (at least 80% of markers
successfully genotyped and not duplicated). 

```{r SetupRuns, eval=T, echo=F}
#starting proportions for the chains
### Starting proportions of each of the chains is 95% of 1 reporting group
  
  Baseline_Reps<-Baseline %>%
    group_by(repunit) %>%
    select(repunit,collection) %>%
    unique()
  
  pi_init_List<-lapply(1:length(unique(Baseline_Reps$repunit)),function(x)
    Baseline_Reps %>%
      mutate(RepPi = ifelse(repunit==unique(Baseline_Reps$repunit)[x],1,2))%>%
      mutate(pi_init = ifelse(RepPi==1,
                              0.95/(Baseline_Reps %>%
                                          filter(repunit==unique(Baseline_Reps$repunit)[x]) %>%
                                          nrow()),
                              0.05/(Baseline_Reps %>%
                                          filter(repunit!=unique(Baseline_Reps$repunit)[x]) %>%
                                          nrow())) ) %>%
      ungroup() %>%
      select(collection,pi_init))
  
### Prior on stock proportions GCg
# A 1/k prior on baseline populations puts a larger probability on 
# reporting groups with many populations in the baseline. So we use a 
# 1/GCg prior to have a more uniform prior over the reporting groups 

RepColls<-unique(Baseline[,c(2,3)])

G<-length(unique(RepColls$repunit))
PriorRep<-RepColls%>%
  group_by(repunit)%>%
  summarise(nPops=length(unique(collection)))%>%
  mutate(GCg=nPops*G)%>%
  mutate(pi=1/GCg)

prior_GCg<-RepColls%>%
  mutate(pi_param=as.numeric(plyr::mapvalues(x=repunit,
                                  from=PriorRep$repunit,
                                  to=PriorRep$pi,)))%>%
  select(collection,pi_param)

AnalyNames <- Mixtures$collection %>% unlist() %>% unique()
```

```{r, CondModel, eval=F, echo=F}
dir.create(file.path(paste("Output/",CrntYr,"/",CrntSW,"/Rubias_NonBootstrapModel",sep="")))

MCMCreps <- 100000
BurnIn <- MCMCreps/2
chains <- 7
  
  RubiasRes<-list()
  GR_diag<-list()
  GR_Mat_List<-list()
  
set.seed(11)  
Chn2Samp<-sample(1:chains,1)  
  
for (mx in 1:length(AnalyNames)){

MixTemp<-Mixtures %>%
  filter(collection == AnalyNames[mx]) #subset on the mix


#Check for duplicates
if(any(duplicated(MixTemp[,-c(1:4)]))){
  cat("Warning there are duplicated genotypes, one duplicate removed!!!")
  DupRows<-which(duplicated(MixTemp[,-c(1:4)]))
  MixTemp<-MixTemp[-DupRows,]
}

#check to see if any loci are completely ungenotyped
UnGenoLoci<- MixTemp %>%
  reshape2::melt(.,id.vars=c("sample_type","repunit","collection","indiv")) %>%
  mutate(variable = gsub(pattern="\\.1$",replacement="",variable)) %>%
  group_by(variable) %>%
  summarize(NAct = sum(is.na(value)),
            NAprop = sum(is.na(value))/n())
 Loci2Drop<-UnGenoLoci %>% 
   filter(NAprop == 1) %>%
   select(variable) %>%
   unlist() %>%
   as.vector()

MixTemp <- MixTemp %>%
  select(!starts_with(Loci2Drop))

BaseTemp <- Baseline%>%
  select(!starts_with(Loci2Drop))

registerDoFuture()
plan(multisession) #multisession on windows

X <- 1:chains

mix_est_list <- foreach(x = X) %dorng% { #dorng vs dopar
  mix_est <- infer_mixture(reference = BaseTemp, 
                         mixture = MixTemp, 
                         gen_start_col = 5,
                         reps = MCMCreps, 
                         burn_in = BurnIn,
                         pi_init = pi_init_List[[x]],                          
                         pi_prior = prior_GCg)#,
                         #method="PB")
  mix_est

  }

MCMC_chains<-lapply(1:chains,function(x)
mix_est_list[[x]]$mix_prop_traces %>%
  filter(sweep > BurnIn) %>%
  group_by(sweep, repunit) %>%
  summarise(repprop = sum(pi))%>%
  reshape2::dcast(.,formula = sweep ~ repunit , value.var = "repprop") %>%
  select(-sweep)%>%
  coda::mcmc(.)
)%>%
  coda::mcmc.list()

GR_diag <- coda::gelman.diag(MCMC_chains,autoburnin = F, multivariate = F)

GR_Mat_List[[mx]]<-GR_diag$psrf %>%
  as.data.frame() %>%
  mutate(repunit = row.names(.))%>%
  mutate(Analysis=AnalyNames[mx])

RG_mix_ests <- mix_est_list[[Chn2Samp]]$mixing_proportions %>%
  group_by(mixture_collection, repunit) %>%
  summarise(repprop = sum(pi)) 

trace_subset <- mix_est_list[[Chn2Samp]]$mix_prop_traces %>%
  filter(sweep > BurnIn) %>%
  group_by(sweep, repunit) %>%
  summarise(repprop = sum(pi)) 

CI_rub <- trace_subset %>%
  group_by(repunit) %>%
  summarise(loCI = quantile(repprop, probs = 0.025),
            hiCI = quantile(repprop, probs = 0.975))

RubiasRes[[mx]]<-cbind(RG_mix_ests[,2:3],CI_rub[,2:3])%>%
  mutate(Analysis=AnalyNames[mx])%>%
  left_join(.,GR_Mat_List[[mx]],by=c("Analysis","repunit"))

saveRDS(mix_est_list[[Chn2Samp]],file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_NonBootstrapModel/",AnalyNames[mx],".rds",sep="")))

write.csv(RubiasRes[[mx]],
          file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_NonBootstrapModel/",AnalyNames[mx],"_Results.csv",sep="")),
          quote=F,
          row.names=F)

plan(sequential)

}#over mx mixtures

RubiasResMat<-do.call(rbind,RubiasRes)
```

```{r Bootstrap models, eval=F, echo=F}
dir.create(file.path(paste("Output/",CrntYr,"/",CrntSW,"/Rubias_BootstrapModel",sep="")))
#New Code to run BS models  
MCMCreps <- 100000
BURNin <- MCMCreps/2
BootStrapIts<-100  
registerDoFuture()
plan(multisession)


X <- 1:length(AnalyNames)   

ModelRes_List <- foreach(x = X) %dorng% {

MixTemp <- Mixtures%>%
  
  filter(collection==!!AnalyNames[[x]])

if(any(duplicated(MixTemp[,-c(1:4)]))){
  
DupRows<-which(duplicated(MixTemp[,-c(1:4)]))
MixTemp<-MixTemp[-DupRows,]
}

  mix_est <- infer_mixture(reference = Baseline, 
                         mixture = MixTemp, 
                         gen_start_col = 5,
                         reps = MCMCreps, 
                         burn_in = BURNin,
                         pi_init = pi_init_List[[1]],
                         pi_prior = prior_GCg,
                         method = "PB",
                         pb_iter = BootStrapIts)
 
RG_mix_ests <- mix_est$mixing_proportions %>%
  group_by(repunit) %>%
  summarise(repprop = sum(pi)) %>% 
  left_join(mix_est$bootstrapped_proportions) 

BS_diff<- mix_est$mixing_proportions %>%
  group_by(repunit) %>%
  summarise(repprop = sum(pi)) %>% 
  left_join(mix_est$bootstrapped_proportions)%>%
  group_by(repunit)%>%
  summarize(BSdiff = bs_corrected_repunit_ppn - repprop)

trace_subset <- mix_est$mix_prop_traces %>%
  filter(sweep > BURNin) %>%
  group_by(sweep, repunit) %>%
  summarise(repprop = sum(pi))%>%
  mutate(DiffBS = as.numeric(plyr::mapvalues(from=BS_diff$repunit,
                                  to=BS_diff$BSdiff,
                                  x=repunit)))%>%
  mutate(BS_repprop = repprop + DiffBS) 


trace_subset[which(trace_subset$BS_repprop < 0),5]<-0
trace_subset[which(trace_subset$BS_repprop > 1),5]<-1

BS_CI_rub <- trace_subset %>%
  group_by(repunit) %>%
  summarise(loCI_BS = quantile(BS_repprop, probs = 0.025),
            hiCI_BS = quantile(BS_repprop, probs = 0.975),
            sd_BS = sd(BS_repprop),
            median_BS = median(BS_repprop))

CI_rub <- trace_subset %>%
  group_by(repunit) %>%
  summarise(loCI = quantile(repprop, probs = 0.025),
            hiCI = quantile(repprop, probs = 0.975),
            sd = sd(repprop),
            median = median(repprop))

BSMod<-left_join(RG_mix_ests,CI_rub,by="repunit")%>%
  relocate(mixture_collection,.before=repunit)%>%
  relocate(starts_with("bs_corrected_repunit"),.after=median)%>%
  left_join(.,BS_CI_rub,by="repunit")
 
P0 <- trace_subset %>%
  group_by(repunit)%>%
  mutate(LT_1in1Mill = ifelse(BS_repprop < 0.0000005,1,0)) %>%
  summarise(P0 = sum(LT_1in1Mill)/(MCMCreps-BURNin))
 
BSMod<-right_join(BSMod,P0,by="repunit")

write.csv(x=BSMod,
          file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_BootstrapModel/",AnalyNames[[x]],"BSmodRes.csv",sep="")),
          row.names = F) 

saveRDS(mix_est,file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_BootstrapModel/",AnalyNames[[x]],".rds",sep="")))

BSMod
}
```

```{r BSmodelClean, eval=T, echo=F}
GR_Mat<-lapply(1:length(AnalyNames),function(x)
  read_csv(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_NonBootstrapModel/",AnalyNames[[x]],"_Results.csv",sep=""))))%>%
  do.call(rbind,.)%>%
  select(Analysis,repunit,`Point est.`, `Upper C.I.`)%>%
  rename('Region' = "repunit")


BSModRes<-lapply(1:length(AnalyNames),function(x)
  read_csv(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_BootstrapModel/",AnalyNames[[x]],"BSModRes.csv",sep=""))))%>%
  do.call(rbind,.)%>%
  select(mixture_collection,repunit,bs_corrected_repunit_ppn,median_BS,sd_BS,loCI_BS,hiCI_BS,P0)%>%
  rename('Analysis' = "mixture_collection",
         'Region' = "repunit",
         'Mean'="bs_corrected_repunit_ppn",
         'Median' = "median_BS",
         'SD' = "sd_BS",
         '2.5% CI' = "loCI_BS",
         '97.5% CI' = "hiCI_BS",
         'P=0' = "P0")

#Now combine all the results for a table
FullRes <- full_join(BSModRes,GR_Mat,by=c("Analysis","Region"))%>%
   rename('SF' = "Point est.")%>%
   select(-'Upper C.I.')%>%
   mutate(Region = recode(Region,
                        `EAsia`="SE Asia",
                        `NAsia`="NE Asia",
                        `WAlaska` = "W Alaska",
                        `SW_Alaska` = "SW Alaska",
                        `UpMidYukon` = "Up/Mid Yukon",
                        `E_GOP_PNW` = "E GOA/PNW",
                        `KotzebueSound` = "Kotzebue Sound"))%>%
   relocate(SD,.after=Mean)%>%
   relocate(Median,.after='2.5% CI')

MixSize<-Mixtures %>% 
   group_by(collection) %>% 
   summarize(MixSize=n())

PSC_num <- data.frame(Analysis = unlist(AnalyNames))%>%
                        left_join(., (AKRO_BBSRI_Dlvry %>%
                                      filter(SW %in% gsub(pattern="SW",replacement = "",unlist(AnalyNames)))%>%
                                      group_by(SW)%>%
                                      summarise(PSC = sum(CHUM_RETENTION_COUNT,na.rm=T))%>%
                                      mutate(Analysis = paste('SW',SW,sep=""))
                        ))%>%
  left_join(.,Mixtures %>% group_by(collection) %>% summarize(Genetic = n()) %>% rename(Analysis = collection))%>%
  mutate(LabelTable = paste("Stat Week ",SW,sep=""))

for (an in 1:length(AnalyNames)){
AnlyName<-AnalyNames%>%
   unlist()%>%
   . [an]
 
PSCtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(PSC)%>%
   unlist() %>%
  as.numeric()

Mixtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(Genetic)%>%
   unlist()

Labtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(LabelTable)%>%
   unlist()
   
PrtyNm3<-function(x) formatC(x,digits=3,format="f",drop0trailing = F)
PrtyNm2<-function(x) formatC(x,digits=2,format="f",drop0trailing = F)


Res_temp<-FullRes %>%
   filter(Analysis == AnlyName)%>%
   mutate(EstNum = PSCtemp*Mean,
          EstCI = paste(formatC(round(PSCtemp*`2.5% CI`),big.mark=",",format="d"),
                        formatC(round(PSCtemp*`97.5% CI`),big.mark = ",",format="d"),sep="-"))%>%
   relocate(EstNum,.after=Region)%>%
      relocate(EstCI,.after=EstNum)%>%
      select(-c(SD,Median))%>%
   mutate(Region_f = factor(Region,
                    levels=c("SE_Asia","NE_Asia","CWAK","UpperYukon","KotzebueSound","SWAK","GOA/PNW")))%>%
   arrange(Region_f) %>%
  select(-Region_f) %>%
  mutate(Region = recode(Region,
                         "SE_Asia" = 'SE Asia' ,
                         "NE_Asia" = 'NE Asia' ,
                         "CWAK" = 'W Alaska',
                         "UpperYukon" = 'Up/Mid Yukon',
                         "KotzebueSound" = 'Kotzebue Sound',
                         "SWAK" = 'SW Alaska',
                         "GOA/PNW" = 'E GOA/PNW' ))
   
TabTitle<-paste(Labtemp,
                paste(" (PSC = ",formatC(PSCtemp,big.mark=",",format="d"),sep=""),
                "; ",
                paste("n = ",formatC(Mixtemp,big.mark=",",format="d") 
                      ,sep=""),
                ")",
          sep="")

Res_temp %>%
   select(-Analysis)%>%
   mutate(EstNum = formatC(EstNum,big.mark=",",format="d"))%>%
   rename('Est. num.' = "EstNum",
          'Est. CI'="EstCI")%>%
   rename('97.5%' = "97.5% CI")%>%
   rename('2.5%' = "2.5% CI")%>%
   mutate_at(c("Mean","2.5%","97.5%"),PrtyNm3)%>%
   mutate_at(c("P=0","SF"),PrtyNm2)%>%
knitr::kable(.,digits=c(0,0,3,3,3,3,3,3,2),caption=TabTitle,"html",align=c("l","r",rep("c",times=7))) %>%
  kableExtra::kable_classic(full_width = F,html_font = "Times New Roman")%>%
   kableExtra::column_spec(8,background=ifelse(Res_temp$`P=0`>=0.5,"lightgrey","white"))%>%
   kableExtra::save_kable(paste("./Output/",CrntYr,"/",CrntSW,"/",AnlyName,"_TableAltVrs.png",sep=""),zoom=10)
 
}
```

```{r SWresTab, echo = FALSE, message=FALSE,out.height='3in',out.width='4.5in',fig.align = 'center'}
#dt("PP2", "Continued: Primer Probe file for chum salmon GTseq panel.")
knitr::include_graphics(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","SW",CrntSW,"_TableAltVrs.png",sep="")))
```

In statistical week `r CrntSW`, `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% slice_max(Mean) %>% select(Region) %>% unlist()` comprised the largest proportion of the chum salmon bycatch (`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% slice_max(Mean) %>% select(Mean) %>% unlist() %>% {.*100} %>% round(.,1)`%). 

`r if(any(Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% select(EstNum) %>% round(.,0) %>% {.==0})){paste("No chum salmon were caught from the ", Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% mutate(EstNum = round(EstNum,0)) %>% filter(EstNum == 0) %>% select(Region) %>% unlist() %>% paste(.,collapse=", ") %>% {sub(",([^,]*)$", " &\\1", .)},sep="")}`

the largest  a total of - Total number of Western Alaska, Kotz Sound, Up/Mid Yukon
- Total number of Asia fish
- 

\newpage
# References
<div id="refs"></div>

\newpage

# Appendix I - `r CrntYr` GSI Results 
Weekly stock composition estimates of chum salmon bycatch from 
the `r CrntYr` shoreside sector's Bering Sea and Aleutian Islands, 
B-season pollock trawl fishery.

```{r AppI,eval=T,echo=F , results='asis',out.width='6.5in'}
files <- list.files(path = file.path(paste("./Output/",CrntYr,sep="")), recursive = T,pattern = "png", full.names = TRUE)%>%
  grep(pattern="AltVrs",x=.,invert=F,value=T)
  

for (f in files) {
  cat(paste0("![](", f, "){#id .class width=6in} \n
             \n
             \n
             \n"))
  }
```

\newpage

\blandscape
# Appendix II - Genetic loci information
```{r PP1, echo = FALSE, message=FALSE,out.height='6.5in',out.width='9in',fig.align = 'left'}
#dt("PP1", "Primer Probe file for chum salmon GTseq panel.")
knitr::include_graphics("./MainData/LocusSummaryTab_1-42.png")
```
\newpage
```{r PP2, echo = FALSE, message=FALSE,out.height='6.5in',out.width='9in',fig.align = 'left'}
#dt("PP2", "Continued: Primer Probe file for chum salmon GTseq panel.")
knitr::include_graphics("./MainData/LocusSummaryTab_43-84.png")
```
\elandscape


