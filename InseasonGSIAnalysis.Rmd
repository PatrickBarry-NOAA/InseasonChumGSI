---
title: Inseason analysis of chum salmon *Oncorhynchus* *keta* bycatch from shoreside
  sector of Bering Sea Aleutian Islands walleye pollock *Gadus* *chalcogrammus* trawl
  fishery.
author: Patrick Barry^[NOAA Alaska Fisheries Science Center, patrick.barry@noaa.gov],
  Jamie Musbach, and Wes Larson.
date: "`r paste('Report generated on: ', Sys.Date(),sep='')`"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)

CrntSW <- 28 #Current Stat week
CrntYr <- format(Sys.Date(),"%Y") #Get the current year

dir.create(file.path(paste("Output/",CrntYr,"/",CrntSW,sep="")))

library(tidyverse)

#functions
statweek = function(dates, format="%d-%m-%Y", ...) {
  # convert to Date
  dates = as.Date(dates, format=format, ...) 
  # get correction for the first week of the year (0 if 1-Jan not a Sunday)
  firstweek = 1 - as.numeric(format(as.Date(cut(dates, "year")), "%U")) 
  output = as.numeric(format(dates, "%U")) + firstweek
  return(output)
}
```

---
subtitle: '`r paste("Results from Statistical Week", CrntSW, sep="")`'
---

\newpage


# Summary

Main results from analysis w/ table

\newpage

# Introduction
```{r readAKRO, eval=T, echo=F}
AKRO_Files <- list.files(file.path(paste("./MainData/AKRO/",CrntYr,sep="")),
                         include.dirs = FALSE) #list AKRO files sent by Glenn

#Find the file to process
FileWeeks <- AKRO_Files %>%
  str_split(.,"_")%>%
  lapply(.,"[[",1)%>%
  unlist() %>% 
  as.Date(.,format="%Y%m%d")%>%
  format(.,"%d-%m-%Y")%>%
  statweek(.)

File2Read <- AKRO_Files[which.min(abs(CrntSW-FileWeeks)) ]

AKRO_Obs <- readxl::read_xls(file.path(paste("./MainData/AKRO/",CrntYr,"/",File2Read,sep=""))) %>%
  mutate(DATE = format(DELIVERY_END_DATE,"%d-%m-%Y"))%>%
  mutate(SW = statweek(DATE))

AKRO_ObsSum <- AKRO_Obs %>%
  group_by(SW,NMFS_AREA) %>%
  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
            nDeliveries = n(),
            nUniqVess = length(unique(DELIVERING_VESSEL_PERMIT)))%>%
  mutate(nChumPerDeliv = nChum/nDeliveries)%>%
    filter(nUniqVess >=3)

```

Within the Federally Managed midwater trawl fishery for walleye 
pollock (*Gadus* *chalcogrammus*) in the Bering Sea and Aleutian 
Islands (BSAI), chum salmon (*Oncorhynchus* *keta*) incidental catch occurs occasionally in very 
large numbers. Salmon are managed as a prohibited species catch (bycatch) 
and are highly regulated. Currently, 
annual estimates of genetic stock composition are produced by NOAA's 
Alaska Fishery Science Center (AFSC) genetics program and presented to 
the North Pacific Fisheries Management Council (NPFMC). All chum salmon bycatch
is enumerated by the observer program and 1 in 30 are sampled; information 
taken on length, weight, sex, and a tissue sample and scale sent to the AFSC 
genetics program for analysis. In 2024 a project was initiated by Bristol Bay
Salmon Research Institute (BBSRI) to sample the bycatch from the shore based 
sector of the fleet in order to obtain weekly estimates of genetic stock 
composition. This report outlines the results the analysis of the chum
salmon bycatch from statistical week `r CrntSW` (`r AKRO_Obs%>% filter(SW == CrntSW) %>% slice_min(order_by=DATE)%>% slice_head(n=1) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b")` - 
`r AKRO_Obs%>% filter(SW == min(AKRO_ObsSum$SW)) %>% slice_max(order_by=DATE)%>% slice_head(n=1) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b")`).

# Results 
## Chum Salmon Bycatch  

```{r AKROsummary, eval=T, echo=F}

AKRO_ObsSum <- bind_rows(AKRO_ObsSum %>% mutate(NMFS_AREA = as.character(NMFS_AREA)),
                         AKRO_Obs %>%
                          group_by(SW) %>%
                          summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
                                    nDeliveries = n(),
                                    nUniqVess = length(unique(DELIVERING_VESSEL_PERMIT)))%>%
                          mutate(nChumPerDeliv = nChum/nDeliveries)%>%
                           mutate(NMFS_AREA = "BSAI Total")%>%
                            filter(nUniqVess >3))
                         
  
```

Since statistical week `r min(AKRO_Obs$SW)` (`r AKRO_Obs%>% filter(SW == min(AKRO_ObsSum$SW)) %>% slice_min(order_by=DATE) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b")`),  `r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% ungroup() %>% select(nChum) %>% sum() %>% formatC(.,big.mark = ",")` chum salmon have been incidentally
caught by the shoreside sector. In the current statistical 
week (`r CrntSW`), there were 
`r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum() %>% formatC(.,big.mark = ",")` chum salmon caught in `r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nDeliveries) %>% sum() %>% formatC(.,big.mark = ",")  ` deliveries, `r ifelse((AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) < (AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==(CrntSW-1))%>%ungroup() %>% select(nChum) %>% sum()),"a decrease in total bycatch","an increase in total bycatch")` from the previous week (Figure 1). The majority of chum salmon were caught in NMFS Area `r AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA)%>% unlist()`. `r ifelse(AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA) == AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),paste("Similarly, the largest chum rate (chum per delivery) was also in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),sep=""),paste("Despite the largest number of chum salmon being caught in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA)," the highest rate of chum salmon bycatch was in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),sep=""))`.

```{r AKRO_Bycatch, eval=T, echo=F, fig.cap="Number of chum salmon caught by the shoreside sector of the Bering Sea pollock trawl fishery by statistical week. Weekly totals for which fewer than 4 vessels made deliveries have been omitted.",fig.width=6,fig.height=3}
AKRO_ObsSum %>%
  ggplot(.,aes(x=SW,y=nChum,color=as.factor(NMFS_AREA),group=NMFS_AREA))+
  geom_point()+
  geom_line()+
  theme_bw()+
  ylab("Number of Chum Salmon")+
  xlab("Statistical Week")+
  guides(color=guide_legend(title="NMFS\nStat Area"))+
  scale_y_continuous(labels=scales::comma)

```

## Sampling of Bycatch

```{r BBSRIsampling, eval=T, echo=F}
BBSRI_Dlvry <- readxl::read_xlsx(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/","Catch Sampling_Master_SW",CrntSW,".xlsx",sep="")),sheet = 'DELIVERYINFO') %>%
  mutate(DATE = as.Date(as.character(DATE),"%Y-%m-%d"))%>%
  mutate(DATE2 = format(DATE,"%d-%m-%Y"))%>%
  mutate(SW = statweek(DATE2))%>%
  rename('PROCESSOR_PERMIT' = 'PROCESSOR',
         'OFFLOAD_NUMBER' = 'OFFLOAD_NO')

#Bind to obs data
AKRO_BBSRI_Dlvry <- AKRO_Obs %>%
  full_join(.,BBSRI_Dlvry, by = c('CRUISE','PROCESSOR_PERMIT',"OFFLOAD_NUMBER","SW"))%>% #don't merge by NMFS area, use NMFS_AREA.x from AKRO data in subsequent code
  filter(SW == CrntSW)

if(any(is.na(AKRO_BBSRI_Dlvry$CRUISE) & (AKRO_BBSRI_Dlvry %>% filter(is.na(CRUISE)) %>% select(CHUM_LANDED) %>% sum(.,na.rm=T) %>% {.>0}))==TRUE) {
  "WARNING: Positive chum catches for cruises not in AKRO data"
}else{ AKRO_BBSRI_Dlvry <- AKRO_BBSRI_Dlvry %>% filter(!is.na(CRUISE))
}

```

Of the `r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum() %>% formatC(.,big.mark = ",")` chum salmon caught in 
statistical week `r CrntSW`, a total of `r AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T)`
were sampled for an overall rate of sampling of 
`r (AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% round(.,2)` or 
~`r as.character((AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% janitor::round_to_fraction(.,denominator = 30) %>% MASS::fractions(.))` which 
is `r ifelse((AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum())  < (1/min(as.numeric(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE),na.rm=T)), "less than", "greater than")` the target rate of `r as.character(1/min(as.numeric(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE),na.rm=T) %>% janitor::round_to_fraction(.,denominator = 30) %>% MASS::fractions(.))` (`r (1/min(as.numeric(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE),na.rm=T) %>% janitor::round_to_fraction(.,denominator = 30) %>% MASS::fractions(.))`). `r ifelse(AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE) & CHUM_RETENTION_COUNT > 0) %>% select(CHUM_RETENTION_COUNT) %>% sum(.,na.rm=T) %>% {.>0},paste("There were ", AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE) & CHUM_RETENTION_COUNT > 0) %>% select(CHUM_RETENTION_COUNT) %>% sum(.,na.rm=T), " chum salmon unsampled from ", AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE)  & CHUM_RETENTION_COUNT > 0) %>% nrow(.), " deliveries",sep=""),"All deliveries with non-zero chum bycatch were sampled")`.   

```{r, SampTable, eval=T, echo=F}
options(knitr.kable.NA = '')

AKRO_BBSRI_Dlvry %>%
  group_by(PLANT_NAME,SW,TARGET_SAMPLE_RATE) %>%
  mutate(TARGET_SAMPLE_RATE = round(1/TARGET_SAMPLE_RATE,3))%>%
  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
            nChumSamp = sum(CHUM_SAMPLED,na.rm=T),
            RlzdSampRt = round(nChumSamp / nChum,3))%>%
bind_rows(.,data.frame(PLANT_NAME = "Total",nChum = sum(AKRO_BBSRI_Dlvry$CHUM_RETENTION_COUNT,na.rm=T),nChumSamp = sum(AKRO_BBSRI_Dlvry$CHUM_SAMPLED,na.rm=T),RlzdSampRt = round(sum(AKRO_BBSRI_Dlvry$CHUM_SAMPLED,na.rm=T)/sum(AKRO_BBSRI_Dlvry$CHUM_RETENTION_COUNT,na.rm=T),2)))%>%
  filter(nChum>0)%>%ungroup()%>%
  select(-SW) %>%
  knitr::kable(col.names=c("Plant","Target Sample\nRate","Total\nChum","Chum\nSampled","Sample\nRate"), booktabs = T, caption = paste("Sampling information for statistical week " , CrntSW, " over the shoreside processing plants.",sep=""))%>%
 kableExtra::row_spec(nrow(AKRO_BBSRI_Dlvry %>%
                  group_by(PLANT_NAME,SW,TARGET_SAMPLE_RATE) %>%
                  mutate(TARGET_SAMPLE_RATE = round(1/TARGET_SAMPLE_RATE,3))%>%
                  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
                            nChumSamp = sum(CHUM_SAMPLED,na.rm=T),
                            RlzdSampRt = round(nChumSamp / nChum,3)))-1,hline_after=T)
```


## Genotyping 


```{r GTdat, eval=T, echo=F}
GenoFiles <- list.files(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/",sep="")))%>%
  grep(pattern='Chip|Run',.,value=T)

GenoMetaDat <- readxl::read_xlsx(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/","Catch Sampling_Master_SW28.xlsx",sep="")),
                                sheet = "BIOSAMPLING") 

Genos_Fldm<-lapply(1:length(GenoFiles),function(x) read_delim(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/",GenoFiles[x],sep="")),delim=",",skip = 16,col_names = F) %>%
         rename('Chamber'= 1 ,
                'Locus' = 2,
                'Allele_x' = 3,
                'Allele_y' = 4,
                'Sample_Name' = 5,
                'Sample_Type' = 6,
                'CallInfo_Auto' = 7,
                'Call_Confidence' = 8,
                'CallInfo_Final' = 9,
                'CallInfo_Converted' = 10,
                'Intensity_x' = 11,
                'Intensity_y' = 12) %>%
           mutate(Chip = str_split(string = GenoFiles[x],pattern="_|Chip") %>% lapply(.,"[[",3)%>% str_trim()%>% unlist()))%>%
  bind_rows(.)%>%
  select(1:12,Chip)

Genos_Fldm_Genotype <- Genos_Fldm %>%
  mutate(CallInfo_Converted = recode(CallInfo_Converted,
                                     'No Call' = "NA:NA",
                                     'NTC' = "NTC:NTC"))%>%
  select(Sample_Name,Chip, Locus,CallInfo_Converted) %>%
  reshape2::dcast(Sample_Name + Chip ~ Locus , value.var= c("CallInfo_Converted"))

rubiasCols <- data.frame(sample_type = "mixture",
                         repunit = NA,
                         collection = "SW28",
                         indiv = Genos_Fldm_Genotype$Sample_Name)

Genos_Fldm_Alleles <- lapply(3:ncol(Genos_Fldm_Genotype),function(x)
  str_split(Genos_Fldm_Genotype[,x],pattern=":") %>%
    {cbind(lapply(.,"[[",1) %>% unlist(),lapply(.,"[[",2) %>% unlist())}
  )%>%
  do.call(cbind,.)%>%
  {cbind(rubiasCols,.)}

colnames(Genos_Fldm_Alleles)[5:(ncol(Genos_Fldm_Alleles))]<-paste(rep(colnames(Genos_Fldm_Genotype)[-c(1:2)],each=2),c("1","2"),sep="_")

```


```{r QC_input, eval=T, echo=F}
#check for samples not genotyped
if(length(GenoMetaDat$`WHAT_CARD-NO`[!GenoMetaDat$`WHAT_CARD-NO`%in%Genos_Fldm_Alleles$indiv])>0){
  paste("WARNING: Samples in biosampling sheet not associated with genotypes (",paste(GenoMetaDat$`WHAT_CARD-NO`[!GenoMetaDat$`WHAT_CARD-NO`%in%Genos_Fldm_Alleles$indiv],collapse=" , "),").",sep="")
} 

#check for samples genotyped multiple times
if(any(duplicated(Genos_Fldm_Alleles$indiv%>% {.[.!="NTC"]}))){
  paste("WARNING: Duplicated samples in genotype file (",paste(Genos_Fldm_Alleles$indiv[duplicated(Genos_Fldm_Alleles$indiv,fromLast = T)] %>% {.[.!="NTC"]},collapse=","),").",sep="")
  }   

```


```{r NameFunkiness, eval=T, echo=F}

Baseline <- read.csv("./MainData/ChumBaseline_ABL84_382pops_42071inds_KotzSnd.csv")

Mixtures <- Genos_Fldm_Alleles

Mixtures[,(2:ncol(Mixtures))]<-lapply(((2:ncol(Mixtures))), function(x) as.character(Mixtures[,x]))

Baseline[,2:ncol(Baseline)]<-lapply(2:ncol(Baseline), function(x) as.character(Baseline[,x]))

Mixtures[Mixtures==0]<-NA

# Confirm that the loci names match  
BaseLoci <- colnames(Baseline)[-(1:4)] %>%
  gsub(pattern="\\.1$|_1$|_2$",replacement="",.) %>%
  gsub(pattern="\\.",replacement="_",.) %>%
  tolower()

 colnames(Baseline)[-(1:4)] <- paste(BaseLoci,c("",".1"),sep="")

MixLoci <- colnames(Mixtures)[-(1:4)] %>%
  gsub(pattern="_1$|_1.1$|_2$",replacement="",.) %>%
  gsub(pattern="-",replacement="_",.) %>%
  tolower()
  
colnames(Mixtures)[-(1:4)] <- paste(MixLoci,c("",".1"),sep="")

Loci2RM <- MixLoci[!MixLoci %in% BaseLoci][c(T,F)]

Mixtures <- Mixtures %>%
  select(-(contains(Loci2RM)))

BaseLoci <- colnames(Baseline)[-(1:4)]
MixLoci <- colnames(Mixtures)[-(1:4)]

if(any(BaseLoci != MixLoci)){ 
  LociOrder <- lapply(BaseLoci[c(T,F)],function(BL) which(MixLoci %in% BL)) %>%
    unlist() +4
} else {
    LociOrder <- 5:(84*2+4)
  }

Mixtures<-Mixtures[,c(1:4,LociOrder)]

if(any(colnames(Baseline)!= colnames(Mixtures))){
  "WARNING!!!! rubias baseline and mixture files have different locus order."
}

#remove NTC sample
Mixtures <- Mixtures %>% 
  filter(indiv != "NTC")

# close_matching_samples(Mixtures,
#                               gen_start_col = 5)

```


```{r SetupRuns, eval=T, echo=F}
#starting proportions for the chains
### Starting proportions of each of the chains is 95% of 1 reporting group
  
  Baseline_Reps<-Baseline %>%
    group_by(repunit) %>%
    select(repunit,collection) %>%
    unique()
  
  pi_init_List<-lapply(1:length(unique(Baseline_Reps$repunit)),function(x)
    Baseline_Reps %>%
      mutate(RepPi = ifelse(repunit==unique(Baseline_Reps$repunit)[x],1,2))%>%
      mutate(pi_init = ifelse(RepPi==1,
                              0.95/(Baseline_Reps %>%
                                          filter(repunit==unique(Baseline_Reps$repunit)[x]) %>%
                                          nrow()),
                              0.05/(Baseline_Reps %>%
                                          filter(repunit!=unique(Baseline_Reps$repunit)[x]) %>%
                                          nrow())) ) %>%
      ungroup() %>%
      select(collection,pi_init))
  
### Prior on stock proportions GCg
# A 1/k prior on baseline populations puts a larger probability on 
# reporting groups with many populations in the baseline. So we use a 
# 1/GCg prior to have a more uniform prior over the reporting groups 

RepColls<-unique(Baseline[,c(2,3)])

G<-length(unique(RepColls$repunit))
PriorRep<-RepColls%>%
  group_by(repunit)%>%
  summarise(nPops=length(unique(collection)))%>%
  mutate(GCg=nPops*G)%>%
  mutate(pi=1/GCg)

prior_GCg<-RepColls%>%
  mutate(pi_param=as.numeric(plyr::mapvalues(x=repunit,
                                  from=PriorRep$repunit,
                                  to=PriorRep$pi,)))%>%
  select(collection,pi_param)

AnalyNames <- Mixtures$collection %>% unlist() %>% unique()
```

```{r, CondModel, eval=F, echo=F}
dir.create(file.path(paste("Output/",CrntYr,"/",CrntSW,"/Rubias_NonBootstrapModel",sep="")))

MCMCreps <- 100000
BurnIn <- MCMCreps/2
chains <- 7
  
  RubiasRes<-list()
  GR_diag<-list()
  GR_Mat_List<-list()
  
set.seed(11)  
Chn2Samp<-sample(1:chains,1)  
  
for (mx in 1:length(AnalyNames)){

MixTemp<-Mixtures %>%
  filter(collection == AnalyNames[mx]) #subset on the mix


#Check for duplicates
if(any(duplicated(MixTemp[,-c(1:4)]))){
  cat("Warning there are duplicated genotypes, one duplicate removed!!!")
  DupRows<-which(duplicated(MixTemp[,-c(1:4)]))
  MixTemp<-MixTemp[-DupRows,]
}

#check to see if any loci are completely ungenotyped
UnGenoLoci<- MixTemp %>%
  reshape2::melt(.,id.vars=c("sample_type","repunit","collection","indiv")) %>%
  mutate(variable = gsub(pattern="\\.1$",replacement="",variable)) %>%
  group_by(variable) %>%
  summarize(NAct = sum(is.na(value)),
            NAprop = sum(is.na(value))/n())
 Loci2Drop<-UnGenoLoci %>% 
   filter(NAprop == 1) %>%
   select(variable) %>%
   unlist() %>%
   as.vector()

MixTemp <- MixTemp %>%
  select(!starts_with(Loci2Drop))

BaseTemp <- Baseline%>%
  select(!starts_with(Loci2Drop))

registerDoFuture()
plan(multisession) #multisession on windows

X <- 1:chains

mix_est_list <- foreach(x = X) %dorng% { #dorng vs dopar
  mix_est <- infer_mixture(reference = BaseTemp, 
                         mixture = MixTemp, 
                         gen_start_col = 5,
                         reps = MCMCreps, 
                         burn_in = BurnIn,
                         pi_init = pi_init_List[[x]],                          
                         pi_prior = prior_GCg)#,
                         #method="PB")
  mix_est

  }

MCMC_chains<-lapply(1:chains,function(x)
mix_est_list[[x]]$mix_prop_traces %>%
  filter(sweep > BurnIn) %>%
  group_by(sweep, repunit) %>%
  summarise(repprop = sum(pi))%>%
  reshape2::dcast(.,formula = sweep ~ repunit , value.var = "repprop") %>%
  select(-sweep)%>%
  coda::mcmc(.)
)%>%
  coda::mcmc.list()

GR_diag <- coda::gelman.diag(MCMC_chains,autoburnin = F, multivariate = F)

GR_Mat_List[[mx]]<-GR_diag$psrf %>%
  as.data.frame() %>%
  mutate(repunit = row.names(.))%>%
  mutate(Analysis=AnalyNames[mx])

RG_mix_ests <- mix_est_list[[Chn2Samp]]$mixing_proportions %>%
  group_by(mixture_collection, repunit) %>%
  summarise(repprop = sum(pi)) 

trace_subset <- mix_est_list[[Chn2Samp]]$mix_prop_traces %>%
  filter(sweep > BurnIn) %>%
  group_by(sweep, repunit) %>%
  summarise(repprop = sum(pi)) 

CI_rub <- trace_subset %>%
  group_by(repunit) %>%
  summarise(loCI = quantile(repprop, probs = 0.025),
            hiCI = quantile(repprop, probs = 0.975))

RubiasRes[[mx]]<-cbind(RG_mix_ests[,2:3],CI_rub[,2:3])%>%
  mutate(Analysis=AnalyNames[mx])%>%
  left_join(.,GR_Mat_List[[mx]],by=c("Analysis","repunit"))

saveRDS(mix_est_list[[Chn2Samp]],file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_NonBootstrapModel/",AnalyNames[mx],".rds",sep="")))

write.csv(RubiasRes[[mx]],
          file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_NonBootstrapModel/",AnalyNames[mx],"_Results.csv",sep="")),
          quote=F,
          row.names=F)

plan(sequential)

}#over mx mixtures

RubiasResMat<-do.call(rbind,RubiasRes)
```


```{r Bootstrap models, eval=F, echo=T}
dir.create(file.path(paste("Output/",CrntYr,"/",CrntSW,"/Rubias_BootstrapModel",sep="")))
#New Code to run BS models  
MCMCreps <- 100000
BURNin <- MCMCreps/2
BootStrapIts<-100  
registerDoFuture()
plan(multisession)


X <- 1:length(AnalyNames)   

ModelRes_List <- foreach(x = X) %dorng% {

MixTemp <- Mixtures%>%
  
  filter(collection==!!AnalyNames[[x]])

if(any(duplicated(MixTemp[,-c(1:4)]))){
  
DupRows<-which(duplicated(MixTemp[,-c(1:4)]))
MixTemp<-MixTemp[-DupRows,]
}

  mix_est <- infer_mixture(reference = Baseline, 
                         mixture = MixTemp, 
                         gen_start_col = 5,
                         reps = MCMCreps, 
                         burn_in = BURNin,
                         pi_init = pi_init_List[[1]],
                         pi_prior = prior_GCg,
                         method = "PB",
                         pb_iter = BootStrapIts)
 
RG_mix_ests <- mix_est$mixing_proportions %>%
  group_by(repunit) %>%
  summarise(repprop = sum(pi)) %>% 
  left_join(mix_est$bootstrapped_proportions) 

BS_diff<- mix_est$mixing_proportions %>%
  group_by(repunit) %>%
  summarise(repprop = sum(pi)) %>% 
  left_join(mix_est$bootstrapped_proportions)%>%
  group_by(repunit)%>%
  summarize(BSdiff = bs_corrected_repunit_ppn - repprop)

trace_subset <- mix_est$mix_prop_traces %>%
  filter(sweep > BURNin) %>%
  group_by(sweep, repunit) %>%
  summarise(repprop = sum(pi))%>%
  mutate(DiffBS = as.numeric(plyr::mapvalues(from=BS_diff$repunit,
                                  to=BS_diff$BSdiff,
                                  x=repunit)))%>%
  mutate(BS_repprop = repprop + DiffBS) 


trace_subset[which(trace_subset$BS_repprop < 0),5]<-0
trace_subset[which(trace_subset$BS_repprop > 1),5]<-1

BS_CI_rub <- trace_subset %>%
  group_by(repunit) %>%
  summarise(loCI_BS = quantile(BS_repprop, probs = 0.025),
            hiCI_BS = quantile(BS_repprop, probs = 0.975),
            sd_BS = sd(BS_repprop),
            median_BS = median(BS_repprop))

CI_rub <- trace_subset %>%
  group_by(repunit) %>%
  summarise(loCI = quantile(repprop, probs = 0.025),
            hiCI = quantile(repprop, probs = 0.975),
            sd = sd(repprop),
            median = median(repprop))

BSMod<-left_join(RG_mix_ests,CI_rub,by="repunit")%>%
  relocate(mixture_collection,.before=repunit)%>%
  relocate(starts_with("bs_corrected_repunit"),.after=median)%>%
  left_join(.,BS_CI_rub,by="repunit")
 
P0 <- trace_subset %>%
  group_by(repunit)%>%
  mutate(LT_1in1Mill = ifelse(BS_repprop < 0.0000005,1,0)) %>%
  summarise(P0 = sum(LT_1in1Mill)/(MCMCreps-BURNin))
 
BSMod<-right_join(BSMod,P0,by="repunit")

write.csv(x=BSMod,
          file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_BootstrapModel/",AnalyNames[[x]],"BSmodRes.csv",sep="")),
          row.names = F) 

saveRDS(mix_est,file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_BootstrapModel/",AnalyNames[[x]],".rds",sep="")))

BSMod
}
```

```{r BSmodelClean, eval=T, echo=F}
GR_Mat<-lapply(1:length(AnalyNames),function(x)
  read_csv(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_NonBootstrapModel/",AnalyNames[[x]],"_Results.csv",sep=""))))%>%
  do.call(rbind,.)%>%
  select(Analysis,repunit,`Point est.`, `Upper C.I.`)%>%
  rename('Region' = "repunit")


BSModRes<-lapply(1:length(AnalyNames),function(x)
  read_csv(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_BootstrapModel/",AnalyNames[[x]],"BSModRes.csv",sep=""))))%>%
  do.call(rbind,.)%>%
  select(mixture_collection,repunit,bs_corrected_repunit_ppn,median_BS,sd_BS,loCI_BS,hiCI_BS,P0)%>%
  rename('Analysis' = "mixture_collection",
         'Region' = "repunit",
         'Mean'="bs_corrected_repunit_ppn",
         'Median' = "median_BS",
         'SD' = "sd_BS",
         '2.5% CI' = "loCI_BS",
         '97.5% CI' = "hiCI_BS",
         'P=0' = "P0")

#Now combine all the results for a table
FullRes <- full_join(BSModRes,GR_Mat,by=c("Analysis","Region"))%>%
   rename('SF' = "Point est.")%>%
   select(-'Upper C.I.')%>%
   mutate(Region = recode(Region,
                        `EAsia`="SE Asia",
                        `NAsia`="NE Asia",
                        `WAlaska` = "W Alaska",
                        `SW_Alaska` = "SW Alaska",
                        `UpMidYukon` = "Up/Mid Yukon",
                        `E_GOP_PNW` = "E GOA/PNW",
                        `KotzebueSound` = "Kotzebue Sound"))%>%
   relocate(SD,.after=Mean)%>%
   relocate(Median,.after='2.5% CI')

MixSize<-Mixtures %>% 
   group_by(collection) %>% 
   summarize(MixSize=n())

PSC_num <- data.frame(Analysis = unlist(AnalyNames))%>%
                        left_join(., (AKRO_BBSRI_Dlvry %>%
                                      filter(SW %in% gsub(pattern="SW",replacement = "",unlist(AnalyNames)))%>%
                                      group_by(SW)%>%
                                      summarise(PSC = sum(CHUM_RETENTION_COUNT,na.rm=T))%>%
                                      mutate(Analysis = paste('SW',SW,sep=""))
                        ))%>%
  left_join(.,Mixtures %>% group_by(collection) %>% summarize(Genetic = n()) %>% rename(Analysis = collection))%>%
  mutate(LabelTable = paste("Stat Week ",SW,sep=""))

for (an in 1:length(AnalyNames)){
AnlyName<-AnalyNames%>%
   unlist()%>%
   . [an]
 
PSCtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(PSC)%>%
   unlist() %>%
  as.numeric()

Mixtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(Genetic)%>%
   unlist()

Labtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(LabelTable)%>%
   unlist()
   
PrtyNm3<-function(x) formatC(x,digits=3,format="f",drop0trailing = F)
PrtyNm2<-function(x) formatC(x,digits=2,format="f",drop0trailing = F)


Res_temp<-FullRes %>%
   filter(Analysis == AnlyName)%>%
   mutate(EstNum = PSCtemp*Mean,
          EstCI = paste(formatC(round(PSCtemp*`2.5% CI`),big.mark=",",format="d"),
                        formatC(round(PSCtemp*`97.5% CI`),big.mark = ",",format="d"),sep="-"))%>%
   relocate(EstNum,.after=Region)%>%
      relocate(EstCI,.after=EstNum)%>%
      select(-c(SD,Median))%>%
   mutate(Region_f = factor(Region,
                    levels=c("SE_Asia","NE_Asia","CWAK","UpperYukon","KotzebueSound","SWAK","GOA/PNW")))%>%
   arrange(Region_f) %>%
  select(-Region_f) %>%
  mutate(Region = recode(Region,
                         "SE_Asia" = 'SE Asia' ,
                         "NE_Asia" = 'NE Asia' ,
                         "CWAK" = 'W Alaska',
                         "UpperYukon" = 'Up/Mid Yukon',
                         "KotzebueSound" = 'Kotzebue Sound',
                         "SWAK" = 'SW Alaska',
                         "GOA/PNW" = 'E GOA/PNW' ))
   
TabTitle<-paste(Labtemp,
                paste(" (PSC = ",formatC(PSCtemp,big.mark=",",format="d"),sep=""),
                "; ",
                paste("n = ",formatC(Mixtemp,big.mark=",",format="d") 
                      ,sep=""),
                ")",
          sep="")

Res_temp %>%
   select(-Analysis)%>%
   mutate(EstNum = formatC(EstNum,big.mark=",",format="d"))%>%
   rename('Est. num.' = "EstNum",
          'Est. CI'="EstCI")%>%
   rename('97.5%' = "97.5% CI")%>%
   rename('2.5%' = "2.5% CI")%>%
   mutate_at(c("Mean","2.5%","97.5%"),PrtyNm3)%>%
   mutate_at(c("P=0","SF"),PrtyNm2)%>%
knitr::kable(.,digits=c(0,0,3,3,3,3,3,3,2),caption=TabTitle,"html",align=c("l","r",rep("c",times=7))) %>%
  kableExtra::kable_classic(full_width = F,html_font = "Times New Roman")%>%
   kableExtra::column_spec(8,background=ifelse(Res_temp$`P=0`>=0.5,"lightgrey","white"))%>%
   kableExtra::save_kable(paste("./Output/",CrntYr,"/",CrntSW,"/",AnlyName,"_TableAltVrs.png",sep=""),zoom=10)
 
}
```




# Appendix I: Methods
## Genotyping
## Genetic Stock Composition Estimates
