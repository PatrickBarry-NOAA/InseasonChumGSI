---
title: Inseason analysis of chum salmon (*Oncorhynchus* *keta*) bycatch from the shoreside
  sector of the Bering Sea Aleutian Islands walleye pollock (*Gadus* *chalcogrammus*)
  trawl fishery
author: "Patrick Barry, Jamie Musbach, Abby Duffy, and Wes Larson"
date: "`r paste('Report generated on: ', Sys.Date(),sep='')`"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
    includes:
      in_header: ./latex_fmt/header.tex
bibliography: ./Bib/NOAA_GSI.bib
csl: ./Bib/american-fisheries-society.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)

#CrntSW <- 33 # optionally render manually by setting SW
CrntYr <- format(Sys.Date(),"%Y") #Get the current year

#dir.create(file.path(paste("Output/",CrntYr,"/",CrntSW,sep="")))

library(tidyverse)
library(rubias)
library(doFuture)
library(doRNG)
library(sf)
library(ggsn)
library(marmap)
#library(inlmisc)

#functions
statweek = function(dates, format="%d-%m-%Y", ...) {
  # convert to Date
  dates = as.Date(dates, format=format, ...) 
  # get correction for the first week of the year (0 if 1-Jan not a Sunday)
  firstweek = 1 - as.numeric(format(as.Date(cut(dates, "year")), "%U")) 
  output = as.numeric(format(dates, "%U")) + firstweek
  return(output)
}

#Dummy table function
dt = function(label, caption=NULL) {
  print(xtable::xtable(setNames(data.frame(x=numeric()), " "),
               caption=caption,
               label=paste0("tab:", label)), 
        hline.after=NULL,
        booktabs=FALSE,
        size="\\fontsize{0.1pt}{0.1pt}\\selectfont")
}
```

---
subtitle: '`r paste("Results from Statistical Week ", CrntSW, sep="")`'
---

\newpage


# Summary
```{r exec-summary, echo = FALSE, ref.label = c("readAKRO", "AKROsummary","BBSRIsampling","GTdat","QC_input","NameFunkiness","SampGeno","SetupRuns", "BSmodelClean")}
```

In statistical week `r CrntSW` of the Bering Sea Aleutian Islands (BSAI)
walleye pollock trawl fishery there were
`r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum() %>% formatC(.,big.mark = ",")` chum salmon caught by the shoreside sector. Of that total catch, `r AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T)`
were sampled, `r ProcessRate$nPrcsd %>% sum(.,na.rm=T)` were selected for genotyping, 
and `r ProcessRate$nGenod %>% sum(.,na.rm=T)` (`r (ProcessRate$nGenod %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% {.*100} %>% round(.,1)`% of the total catch)  were successfully genotyped to determine the 
genetic stock composition of the bycatch. `r ifelse(AKRO_BBSRI_DlvrySum$GenoRate[-length(AKRO_BBSRI_DlvrySum)] %>% {.[!is.na(.)]} %>% dist(.) %>% {any(.>0.05)}, "Genotyping rates varied substantially (>5%) across collections (processing plants & target sampling rates) suggesting that the collection of samples may not be representative of the overall bycatch. Samples were analyzed with the understanding that some bias may exist in the spatial and temporal coverage of the samples.", "")` `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% slice_max(Mean) %>% select(Region) %>% unlist()` comprised the largest proportion of the chum salmon bycatch (`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% slice_max(Mean) %>% select(Mean) %>% unlist() %>% {.*100} %>% round(.,1)`%), 
`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% slice_max(EstNum) %>% select(EstNum) %>% unlist() %>% round(.,0)` chum salmon. Western Alaska (Coastal Western Alaska, Kotzebue Sound, 
and Upper/Middle Yukon combined) comprised `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% filter(grepl(pattern = "W Alaska|Kotzebue|Yukon", Region)) %>% select(Mean) %>% sum(.,na.rm=T) %>% unlist() %>% {.*100} %>% round(.,1)`% 
of the bycatch (`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% filter(grepl(pattern = "W Alaska|Kotzebue|Yukon", Region)) %>% select(EstNum) %>% sum(.,na.rm=T) %>% unlist() %>% round(.,0)` fish). `r ifelse(PrntByctchSW==T, paste(" Since statistical week ", min(AKRO_ObsSum$SW,na.rm=T)," a total of ", AKRO_Obs$CHUM_RETENTION_COUNT %>% sum(.,na.rm=T) %>% formatC(.,big.mark=",",format="d"), " chum salmon have been caught by the shoreside sector, with the majority of chum salmon being caught in NMFS area ", AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total") %>% group_by(NMFS_AREA) %>% summarize(nChum = sum(nChum,na.rm=T)) %>% slice_max(nChum) %>% select(NMFS_AREA) %>% unlist(),".",sep=""))`  


\newpage

# Introduction
```{r readAKRO, eval=T, echo=F}
AKRO_Files <- list.files(file.path(paste("./MainData/AKRO/",CrntYr,sep="")),
                         include.dirs = FALSE) #list AKRO files sent by Glenn

#Find the file to process
FileWeeks <- AKRO_Files %>%
  str_split(.,"_")%>%
  lapply(.,"[[",1)%>%
  unlist() %>% 
  as.Date(.,format="%Y%m%d")%>%
  format(.,"%d-%m-%Y")%>%
  statweek(.)

File2Read <- AKRO_Files[length(AKRO_Files)] #AKRO_Files[which.min(abs(CrntSW-FileWeeks)) ]

AKRO_Obs <- readxl::read_xls(file.path(paste("./MainData/AKRO/",CrntYr,"/",File2Read,sep=""))) %>%
  mutate(DATE = format(DELIVERY_END_DATE,"%d-%m-%Y"))%>%
  mutate(SW = statweek(DATE)) %>%
  filter(SW <= CrntSW)

AKRO_ObsSum <- AKRO_Obs %>%
  group_by(SW,NMFS_AREA) %>%
  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
            nDeliveries = n(),
            nUniqVess = length(unique(DELIVERING_VESSEL_PERMIT)))%>%
  mutate(nChumPerDeliv = nChum/nDeliveries)%>%
    filter(nUniqVess >=3)

```

Chum salmon (*Oncorhynchus* *keta*) incidental catch occurs within the Federally Managed midwater trawl fishery for walleye pollock (*Gadus* *chalcogrammus*) in the Bering Sea and Aleutian 
Islands (BSAI). Salmon are managed as a prohibited species catch (referred to as bycatch) 
and are highly regulated. The fishery is composed of three distinct processing sectors, each 
with different operational constraints. Within the shoreside sector, smaller vessels
which lack the ability to process hauls at-sea often fish closer to the Alaska Peninsula. 
Differences in the stock specific distribution of chum salmon within the Bering Sea
result in the shoreside sector often catching the largest proportion and number of Western Alaska (Coastal Western Alaska, Yukon Fall run, and 
Kotzebue Sound genetic groups combined) chum salmon bycatch relative to the catcher processor and mothership sectors [@Kondzelaetal2017].
Currently, annual estimates of genetic stock composition are produced by
the genetics program of NOAA's 
Alaska Fishery Science Center (AFSC) and presented to 
the North Pacific Fisheries Management Council (NPFMC) at their April Council meeting (~3 months
after the end of the B season). Within the fishery all chum salmon bycatch
is enumerated by the North Pacific Observer Program and 1 in 30 are sampled for length, weight, sex, 
and a tissue sample and a scale are sent to the AFSC 
genetics program for analysis. In 2024, a project was initiated by Bristol Bay
Science Research Institute (BBSRI) to sample the bycatch from the shoreside 
sector of the fleet in order to obtain weekly estimates of genetic stock 
composition. This report outlines the results the analysis of the chum
salmon bycatch from statistical week `r CrntSW` (`r AKRO_Obs%>% filter(SW == CrntSW) %>% slice_min(order_by=DATE)%>% slice_head(n=1) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b")` - 
`r AKRO_Obs%>% filter(SW == CrntSW) %>% slice_max(order_by=DATE)%>% slice_head(n=1) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b")`).

```{r Map, eval=T, echo=F, fig.cap='Map of National Marine Fisheries Service management areas within the Bering Sea and Aleutian Islands. Colored areas indicate those areas from which chum salmon bycatch has historically occured.',fig.width=4}

CrustaceanOrange2='#FF8300'
WavesTeal2='#008998'
SeagrassGreen2='#4C9C2E'
UrchinPurple2='#625BC4'
UrchinPurple4='#9A9A9A'

mapbackground <- UrchinPurple4
clusterpalette <- c(CrustaceanOrange2,WavesTeal2,SeagrassGreen2,UrchinPurple2)

#---------------------------------------------------------------------------------------
#  Make Alaska Basemap
#---------------------------------------------------------------------------------------
sf::sf_use_s2(FALSE)
akmap <- sf::read_sf("./MainData/MapData/AKbasemap.shp")#rgdal::readOGR("./MapData/AKbasemap.shp")
#  Convert the object to a ggmap object

ak <- akmap %>%
  filter(NAME == "Alaska")%>%
  sf::st_transform(.,sf::st_crs("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"))%>%
  {st_crop(x=., y = c(xmin = -179.9, xmax= -110,ymin = 35, ymax= 70))}%>%
  st_shift_longitude()

xmin <- 360-161
xmax <- 360-179
ymin <- 53.5
ymax <- 61.5

stat <- sf::read_sf("./MainData/MapData/adfg_stat_areas_simple.shp") %>%
  sf::st_transform(.,sf::st_crs("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0")) %>%
  st_shift_longitude() %>%
 rmapshaper::ms_simplify()
mylabs <- data.frame(id=stat$OBJECTID,stat_area=stat$STAT_AREA)

nmfs <- sf::read_sf("./MainData/MapData/simplenmfs.shp") %>%
  sf::st_transform(.,4236) %>%
  sf::st_transform(.,sf::st_crs("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0")) %>%
  st_shift_longitude()

# extract centroids of the polygons and combine with stat area labels
centroids.df <- stat %>%
  sf::st_centroid() %>% 
  sf::st_geometry()%>%
  unlist()%>%
  {matrix(as.vector(.),ncol=2,byrow=T)}%>%
  as_tibble()%>%
  mutate(id = as.character(stat$OBJECTID))%>%
  rename(long=V1,lat=V2) %>%
  inner_join(mylabs)
  
cvoa <- sf::read_sf("./MainData/MapData/CVOA/CVOAa.shp") %>%
  sf::st_transform(.,4236) %>%
  sf::st_transform(.,sf::st_crs("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"))%>%
  st_shift_longitude()


marmap::getNOAA.bathy(lon1 = -180, lon2 = -150, lat1 = 53, lat2 = 64, resolution=1) -> AKBath
AKBath_xyz <- marmap::as.xyz(AKBath) %>%
  mutate(V1=V1+360)

# function for a color gradient - note the white is under the map
colfunc <- colorRampPalette(c("royalblue4", "gray88", "white"))
 
StatAreaLabs<-read_csv("./MainData/MapData/StatAreaCoords.csv")%>%
  mutate(long = 360+long)

ggplot() + 
  geom_contour(data = AKBath_xyz, 
               aes(x = V1, y = V2, z = V3),
               binwidth = 500, color = "blue", size = 0.1) +
  geom_sf(data = stat, inherit.aes = FALSE,color="grey50",fill=NA,size=0.25)+
  geom_sf(data = nmfs %>% filter(!REP_AREA %in% c(400,514,541,542,524:550,524,600:660)), inherit.aes = FALSE,color="black",aes(fill=as.factor(REP_AREA)),alpha=0.5,size=1)+
  geom_sf(data = nmfs, inherit.aes = FALSE,color="black", fill=NA,alpha=0.5,size=4)+
  geom_sf(data = ak, inherit.aes = FALSE,color=NA,fill=mapbackground)+
  coord_sf(xlim=c(xmin,xmax),ylim=c(ymin,ymax)) + 
  theme_bw() + 
  theme(axis.title=element_blank(),
        legend.position = "bottom") +
  ggspatial::annotation_north_arrow(
    location = "tr", which_north = "true",
    pad_x = unit(0, "in"), pad_y = unit(0.4, "in"),
    style = ggspatial::north_arrow_nautical(
      fill = c("grey40", "white"),
      line_col = "grey20"
    )
  )+
  ggsn::scalebar(data=nmfs,
                 dist = 50, 
                 dist_unit = "nm",
                 transform = TRUE,
                 location="bottomright",
                 height = 0.005, st.dist = 0.0075, st.bottom = TRUE, st.size = 2,
                 anchor=c(x=360-162,y=53.5))+
   geom_text(data=StatAreaLabs,aes(x=long,y=lat,label=Area),size=2)+
   annotate("text",x=360-178.75,y=56.25,label="Bering Sea",color="grey",fontface='italic')+
  guides(fill=FALSE)

```


# Methods
## Bycatch Sampling
Port samplers, employed by BBSRI, sampled pollock hauls delivered to processing
plants in Dutch Harbor and Akutan. The target sampling rate for statistical 
week `r CrntSW` was 
`r ifelse(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE %>% unique() %>% .[!is.na(.)] %>% length() > 1,"variable","fixed")`
with
`r ifelse(AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE %>% unique() %>% .[!is.na(.)] %>% length() > 1, paste("rates ranging from to 1 in ",AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE %>% unique() %>% .[!is.na(.)] %>% max(.), " to 1 in ", AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE %>% unique() %>% .[!is.na(.)] %>% min(.), " chum salmon sampled (Table 1)",sep=""), paste("at 1 in ", AKRO_BBSRI_Dlvry$TARGET_SAMPLE_RATE %>% unique() %>% .[!is.na(.)]," chum salmon sampled (Table 1)",sep=""))`. After NMFS observers 
had processed the offload, BBSRI technicians took a length measurement, scale sample for 
age estimation, and a fin clip for genetic analyses. Fin clips were stapled 
onto a Whatman card labelled with haul-level information from the 
delivery. Scale samples were mounted on gum cards and stored for post-season analysis. Sampling
and genotyping data were sent to the AFSC genetics program for analysis.

## Genotyping
All tissue samples collected were sent to the
BBSRI staffed genetics laboratory in Dutch Harbor 
for processing. Genomic DNA was extracted from dried finclips 
with Macherey-Nagel (Allentown, PA) NucleoSpin Tissue
kits. Extracted DNA was amplified for 96 single nucleotide polymorphism markers (SNPs)
with a Fluidigm (San Francisco, CA) 
BioMark X9 system with 
96.96 Dynamic Array integrated fluidic circuit (IFC). Each of the 9,216 parallel reactions
consisted of 50–500 $\eta g / \mu l$ DNA,
1X Fast GT Sample Loading Reagent (Fluidigm), 
1X TaqMan GTXpress Master Mix (Applied Biosystems), 
10X Custom ABI TaqMan SNP Genotyping Assay (Applied Biosystems), 
1X Assay Loading Reagent (Fluidigm), and 
2.5X ROX Reference Dye (Invitrogen). The temperature profile for amplification was
thermal mixing at 60°C for 10 min and 70°C for 30 min followed by
“Hot-Start” denaturation at 95°C for 2 min and 40 cycles of 
amplification (denaturation at 95°C for 2 s and annealing at 60°C for 20 s). 
After amplification, genotypes were scored with 
BioMark Genotyping Analysis software.

## Genetic Stock Identification
Mixtures were created by grouping sampled fish into 
temporal groups (statistical week) from non-debriefed observer data 
provided by the Alaska Regional Office and linked to genetic samples
by BBSRI. Individual samples with fewer than 80% of their multilocus 
genotype scored were dropped from analyses. Additionally, if
individuals are identified to have matching multilocus genotpes (>95% similarity)
the indviduals with fewer scored loci was dropped. Genetic stock identification 
was performed with the conditional 
genetic stock identification model in the R package *rubias*
[@MoranAnderson2019] following the methods used in NOAA's annual reports.
Briefly, baseline populations were
grouped into seven regions adapted from [-@Gray2010]: Southeast Asia (SE Asia), 
Northeast Asia (NE Asia), Kotzebue Sound, Western Alaska
(W Alaska), Upper/Middle Yukon (Up/Mid Yukon), Southwest 
Alaska (SW Alaska), and the Eastern GOA/Pacific Northwest (E GOA/PNW).
For all estimates, the Dirichlet prior parameters for the 
stock proportions were defined by region to be
1/($GC_g$), where $C_g$ is the number of baseline populations 
in region $g$, and $G$ is the number of regions.
To ensure convergence to the posterior 
distribution, seven separate MCMC chains of 100,000 
iterations (burn-in of 50,000) of the non-bootstrapped model were 
run, which each chain starting at disparate values of stock 
proportions; configured such that for each chain 95% of the 
mixture came from a single designated reporting group (with 
probability equally distributed among the populations within 
that reporting group) and the remaining 5% equally distributed 
among remaining reporting groups.The convergence of chains 
for each reporting group estimate was assessed with the 
Gelman-Rubin statistic [@GelmanRubin1992] estimated with 
the gelman.diag function in the coda library [@Plummer2006] 
within R. Once chain convergence was confirmed, inference 
was conducted with the conditional genetic stock identification 
model with bootstrapping over reporting groups (MCMC chains of 100,000 
iterations, burn-in of 50,000, 100 bootstrap iterations). 

The stock composition estimates were summarized by the mean, 
standard deviation,
median, 95% credible interval (2.5th and 97.5th percentile of the MCMC iterates in the posterior
output), and $P = 0$, which is the probability that a stock composition estimate is effectively zero
[@Munro2012]. The $P = 0$ statistic is the frequency of the last half of the MCMC iterates of
each chain for which the individual regional contribution to the mixture was less than a threshold
of $0.5E^{-6}$. This statistic may be more useful than the credible interval for assessing the presence
or absence of minor stocks. The estimated number of fish for each genetic group, 
and associated uncertainty, is estimated as the mean stock proportion and 95% 
credible intervals multiplied by the total bycatch in a given statistical week.

# Results 
## Chum Salmon Bycatch  

```{r AKROsummary, eval=T, echo=F}

AKRO_ObsSum <- bind_rows(AKRO_ObsSum %>% mutate(NMFS_AREA = as.character(NMFS_AREA)),
                         AKRO_Obs %>%
                          group_by(SW) %>%
                          summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
                                    nDeliveries = n(),
                                    nUniqVess = length(unique(DELIVERING_VESSEL_PERMIT)))%>%
                          mutate(nChumPerDeliv = nChum/nDeliveries)%>%
                           mutate(NMFS_AREA = "BSAI Total")%>%
                            filter(nUniqVess >3))
                         
PrntByctchSW <- ifelse(length(unique(AKRO_ObsSum$SW)) > 1, TRUE,FALSE)
```

`r ifelse(AKRO_BBSRI_Dlvry$SW %>% unique() %>% .[!is.na(.)] %>% length() > 1, paste(" Since statistical week ", min(AKRO_Obs$SW)," (",AKRO_Obs%>% filter(SW == min(AKRO_ObsSum$SW)) %>% slice_min(order_by=DATE) %>% select(DATE)%>%unlist() %>% as.Date(.,"%d-%m-%Y") %>% format(.,"%d %b"),") ",  AKRO_Obs$CHUM_RETENTION_COUNT %>% sum(.,na.rm=T) %>% formatC(.,big.mark = ","), " chum salmon have been incidentally caught by the shoreside sector. ",sep=""),"")`In the current statistical 
week (`r CrntSW`), there were 
`r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum() %>% formatC(.,big.mark = ",")` chum salmon caught in `r AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nDeliveries) %>% sum() %>% formatC(.,big.mark = ",")  ` deliveries`r ifelse((AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) < (AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==(CrntSW-1))%>%ungroup() %>% select(nChum) %>% sum()) & (AKRO_BBSRI_Dlvry$SW %>% unique() %>% .[!is.na(.)] %>% length() > 1),", a decrease in total bycatch from the previous week (Figure 2)",ifelse((AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) > (AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==(CrntSW-1))%>%ungroup() %>% select(nChum) %>% sum()) & (AKRO_BBSRI_Dlvry$SW %>% unique() %>% .[!is.na(.)] %>% length() > 1) ,", an increase in total bycatch from the previous week (Figure 2)",""))`. The majority of chum salmon were caught in NMFS Area `r AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA)%>% unlist()` (Figure 1). `r ifelse(AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA) == AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),paste("Similarly, the largest chum rate (chum per delivery) was also in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),sep=""),paste("Despite the largest number of chum salmon being caught in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChum) %>% select(NMFS_AREA)," the highest rate of chum salmon bycatch was in NMFS Area ",AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total")  %>% filter(SW==CrntSW) %>% ungroup() %>% slice_max(.,order_by=nChumPerDeliv) %>% select(NMFS_AREA),sep=""))`. `r ifelse(PrntByctchSW==T, paste("A total of ", AKRO_ObsSum$nChum %>% sum(.,na.rm=T) %>% formatC(.,big.mark=",",format="d"), " chum salmon have been caught since statistical week ", min(AKRO_ObsSum$SW,na.rm=T),". With the majority of chum salmon being caught in NMFS area ", AKRO_ObsSum %>% filter(NMFS_AREA!="BSAI Total") %>% group_by(NMFS_AREA) %>% summarize(nChum = sum(nChum,na.rm=T)) %>% slice_max(nChum) %>% select(NMFS_AREA) %>% unlist(),".",sep="")) ` 


```{r AKRO_Bycatch, eval=PrntByctchSW, echo=F, fig.cap="Number of chum salmon caught by the shoreside sector of the Bering Sea pollock trawl fishery by statistical week. Weekly totals for which fewer than 4 vessels made deliveries have been omitted.",fig.width=6,fig.height=3}
AKRO_ObsSum %>%
  ggplot(.,aes(x=as.factor(SW),y=nChum,color=as.factor(NMFS_AREA),group=NMFS_AREA))+
  geom_point()+
  geom_line()+
  theme_bw()+
  ylab("Number of Chum Salmon")+
  xlab("Statistical Week")+
  guides(color=guide_legend(title="NMFS\nStat Area"))+
  scale_y_continuous(labels=scales::comma)+
  coord_cartesian(ylim=c(0,max(AKRO_ObsSum$nChum)))

```

## Sampling & Genotyping 

```{r BBSRIsampling, eval=T, echo=F}
BBSRI_Dlvry <- readxl::read_xlsx(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/","Catch_Sampling_Master_SW",CrntSW,".xlsx",sep="")),sheet = 'DELIVERYINFO') %>%
  mutate(DATE = as.Date(as.character(DATE),"%Y-%m-%d"))%>%
  mutate(DATE2 = format(DATE,"%d-%m-%Y"))%>%
  mutate(SW = statweek(DATE2))%>%
  rename('PROCESSOR_PERMIT' = 'PROCESSOR',
         'OFFLOAD_NUMBER' = 'OFFLOAD_NO')

#Bind to obs data
AKRO_BBSRI_Dlvry <- AKRO_Obs %>%
  full_join(.,BBSRI_Dlvry, by = c('CRUISE','PROCESSOR_PERMIT',"OFFLOAD_NUMBER","SW"))%>% #don't merge by NMFS area, use NMFS_AREA.x from AKRO data in subsequent code
  filter(SW == CrntSW)

if(any(is.na(AKRO_BBSRI_Dlvry$CRUISE) & (AKRO_BBSRI_Dlvry %>% filter(is.na(CRUISE)) %>% select(CHUM_LANDED) %>% sum(.,na.rm=T) %>% {.>0}))==TRUE) {
  "WARNING: Positive chum catches for cruises not in AKRO data"
}else{ AKRO_BBSRI_Dlvry <- AKRO_BBSRI_Dlvry %>% filter(!is.na(CRUISE))
}

```

In statistical week `r CrntSW`, `r AKRO_BBSRI_Dlvry %>% select(SAMPLER_INITIALS) %>% unlist()%>% str_split(.,",|/|\\s") %>% unlist() %>% tolower() %>%unique() %>% {.[!is.na(.)]} %>% {.[.!=""]} %>% length()` BBSRI 
technicians sampled hauls delivered to Dutch Harbor and Akutan. A total of `r AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T)%>% formatC(.,big.mark=",",format="d")` chum salmon
were sampled for an overall sampling rate of 
`r (AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% round(.,2)` or 
~`r as.character((AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% janitor::round_to_fraction(.,denominator = 30) %>% MASS::fractions(.))`. `r ifelse(AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE) & CHUM_RETENTION_COUNT > 0) %>% select(CHUM_RETENTION_COUNT) %>% sum(.,na.rm=T) %>% {.>0},paste("There were ", AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE) & CHUM_RETENTION_COUNT > 0) %>% select(CHUM_RETENTION_COUNT) %>% sum(.,na.rm=T), " chum salmon unsampled from ", AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE)  & CHUM_RETENTION_COUNT > 0) %>% nrow(.), ifelse( AKRO_BBSRI_Dlvry %>% filter(is.na(TARGET_SAMPLE_RATE)  & CHUM_RETENTION_COUNT > 0) %>% nrow(.) %>% {.==1}, " delivery", " deliveries"),sep=""),"All deliveries with non-zero chum bycatch were sampled")` (Table 1). `r ifelse(any(0 %in% GenoMetaDat$SEL_TO_GENOTYPE), paste("Not all chum salmon sampled were genotyped. Of the ", AKRO_BBSRI_Dlvry$CHUM_SAMPLED %>% sum(.,na.rm=T)%>% formatC(.,big.mark=",",format="d"), " samples collected, " , ProcessRate$nPrcsd %>% sum(.,na.rm=T), " were selected for genotyping with ",  ProcessRate$nGenod %>% sum(.,na.rm=T)," (",(ProcessRate$nGenod %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% {.*100} %>% round(.,1),"%) successfully genotyped.",sep=""),paste("All chum salmon sampled were processed for genotyping with ", ProcessRate$nGenod %>% sum(.,na.rm=T)," (",(ProcessRate$nGenod %>% sum(.,na.rm=T) / AKRO_ObsSum %>% filter(NMFS_AREA=="BSAI Total")%>% filter(SW==CrntSW)%>%ungroup() %>% select(nChum) %>% sum()) %>% {.*100} %>% round(.,1),"%) successfully genotyped.",sep="")) ` `r ifelse(nrow(CloseMatches)>=1, paste("Of those genotyped samples, ",nrow(CloseMatches), " pair of samples were likely duplicates and one sample was removed before analyzing the mixture.",sep=""),"")`
`r ifelse(AKRO_BBSRI_DlvrySum$GenoRate[-length(AKRO_BBSRI_DlvrySum)] %>% {.[!is.na(.)]} %>% dist(.) %>% {any(.>0.05)}, "Genotype rates by processor and target sampling rate varied by more than 5% suggesting that the collection of samples may not be representative of the overall bycatch. Samples were analyzed with the understanding that some bias may exist in the spatial and temporal coverage of the samples.", "")`

```{r GTdat, eval=T, echo=F}
GenoFiles <- list.files(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/",sep="")))%>%
  grep(pattern='Chip|Run',.,value=T)

GenoMetaDat <- readxl::read_xlsx(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/","Catch_Sampling_Master_SW",CrntSW,".xlsx",sep="")),
                                sheet = "BIOSAMPLING") 

Genos_Fldm<-lapply(1:length(GenoFiles),function(x) read_delim(file.path(paste("./MainData/BBSRI/",CrntYr,"/SW",CrntSW,"/",GenoFiles[x],sep="")),delim=",",skip = 16,col_names = F) %>%
         rename('Chamber'= 1 ,
                'Locus' = 2,
                'Allele_x' = 3,
                'Allele_y' = 4,
                'Sample_Name' = 5,
                'Sample_Type' = 6,
                'CallInfo_Auto' = 7,
                'Call_Confidence' = 8,
                'CallInfo_Final' = 9,
                'CallInfo_Converted' = 10,
                'Intensity_x' = 11,
                'Intensity_y' = 12) %>%
           mutate(Chip = str_split(string = GenoFiles[x],pattern="_|Chip") %>% lapply(.,"[[",4)%>% str_trim()%>% unlist()) %>%
           filter(!grepl(pattern="NTC",Sample_Name)))%>%
  bind_rows(.)%>%
  select(1:12,Chip)

Genos_Fldm_Genotype <- Genos_Fldm %>%
  mutate(CallInfo_Converted = recode(CallInfo_Converted,
                                     'Invalid' = "NA:NA",
                                     'No Call' = "NA:NA",
                                     'NTC' = "NTC:NTC"))%>%
  select(Sample_Name,Chip, Locus,CallInfo_Converted) %>%
  reshape2::dcast(Sample_Name + Chip ~ Locus , value.var= c("CallInfo_Converted"))

rubiasCols <- data.frame(sample_type = "mixture",
                         repunit = NA,
                         collection = paste("SW",CrntSW,sep=""),
                         indiv = Genos_Fldm_Genotype$Sample_Name)

Genos_Fldm_Alleles <- lapply(3:ncol(Genos_Fldm_Genotype),function(x)
  str_split(Genos_Fldm_Genotype[,x],pattern=":") %>%
    {cbind(lapply(.,"[[",1) %>% unlist(),lapply(.,"[[",2) %>% unlist())}
  )%>%
  do.call(cbind,.)%>%
  {cbind(rubiasCols,.)}

colnames(Genos_Fldm_Alleles)[5:(ncol(Genos_Fldm_Alleles))]<-paste(rep(colnames(Genos_Fldm_Genotype)[-c(1:2)],each=2),c("1","2"),sep="_")

Genos_Fldm_Alleles[Genos_Fldm_Alleles=="NA"]<-NA

```


```{r QC_input, eval=T, echo=F}
#check for samples genotyped multiple times
if(any(duplicated(Genos_Fldm_Alleles$indiv %>% {.[.!="NTC"]}))){
  paste("WARNING: Duplicated samples in genotype file (",paste(Genos_Fldm_Alleles$indiv[duplicated(Genos_Fldm_Alleles$indiv,fromLast = T)] %>% {.[.!="NTC"]},collapse=","),").",sep="")
  }   

nDups<- sum(duplicated(Genos_Fldm_Alleles$indiv %>% {.[.!="NTC"]}))

```

```{r NameFunkiness, eval=T, echo=F,warning = FALSE, message = FALSE}

Baseline <- read.csv("./MainData/ChumBaseline_ABL84_382pops_42071inds_KotzSnd.csv")

Mixtures <- Genos_Fldm_Alleles

Mixtures[,(2:ncol(Mixtures))]<-lapply(((2:ncol(Mixtures))), function(x) as.character(Mixtures[,x]))

Baseline[,2:ncol(Baseline)]<-lapply(2:ncol(Baseline), function(x) as.character(Baseline[,x]))

Mixtures[Mixtures==0]<-NA

# Confirm that the loci names match  
BaseLoci <- colnames(Baseline)[-(1:4)] %>%
  gsub(pattern="\\.1$|_1$|_2$",replacement="",.) %>%
  gsub(pattern="\\.",replacement="_",.) %>%
  tolower()

 colnames(Baseline)[-(1:4)] <- paste(BaseLoci,c("",".1"),sep="")

MixLoci <- colnames(Mixtures)[-(1:4)] %>%
  gsub(pattern="_1$|_1.1$|_2$",replacement="",.) %>%
  gsub(pattern="-",replacement="_",.) %>%
  tolower()
  
colnames(Mixtures)[-(1:4)] <- paste(MixLoci,c("",".1"),sep="")

Loci2RM <- MixLoci[!MixLoci %in% BaseLoci][c(T,F)]

Mixtures <- Mixtures %>%
  select(-(contains(Loci2RM)))

BaseLoci <- colnames(Baseline)[-(1:4)]
MixLoci <- colnames(Mixtures)[-(1:4)]

Test<-cbind(BaseLoci,MixLoci)

if(any(BaseLoci != MixLoci)){ 
  LociOrder <- lapply(BaseLoci[c(T,F)],function(BL) which(MixLoci %in% BL)) %>%
    unlist() +4
} else {
    LociOrder <- 5:(84*2+4)
  }

Mixtures<-Mixtures[,c(1:4,LociOrder)]

if(any(colnames(Baseline)!= colnames(Mixtures))){
  "WARNING!!!! rubias baseline and mixture files have different locus order."
}

#remove NTC sample
Mixtures <- Mixtures %>% 
  filter(indiv != "NTC")

#Samples with low genotyping success?
nloci<- (ncol(Mixtures)-4) / 2

BadInds <- Mixtures %>%
  mutate(CntNA = rowSums(is.na(select(., starts_with("oke")))),
          GTrate = round(1-(CntNA / (nloci*2)),2))%>%
  filter(GTrate<0.8)

if(nrow(BadInds)>0){
  Mixtures<- Mixtures %>%
    filter(!(indiv%in%BadInds$indiv))
}


#work around for call to print wihtin close_matching_samples
dump<-capture.output(CloseMatches <- invisible(rubias::close_matching_samples(D = Mixtures %>% 
                                  as_tibble() %>% 
                                  mutate(sample_type = "reference", 
                                    collection = "CrntSW",
                                    repunit = "S"),
                                 gen_start_col = 5,
                                 min_frac_non_miss = 0.8,
                                 min_frac_matching = 0.9) ))

write.csv(x=CloseMatches,file=file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","CloseMatches_Results.csv",sep="")))

LikelyDuplicated <- Mixtures %>%
  filter(indiv %in% c(CloseMatches$indiv_1,CloseMatches$indiv_2))

#This always removes the second ind. we might want to choose which has the more complete genotype
if(nrow(LikelyDuplicated)>=1){
Mixtures <- Mixtures%>%
  filter(!(indiv%in%CloseMatches$indiv_2))
}

# lapply(MixLoci[c(T,F)],function(x) Mixtures %>%
#          select(starts_with(x))%>%
#          unlist()%>%
#          table())


```


```{r SampGeno, eval=T, echo=F}
ProcessRate <- GenoMetaDat %>%
  left_join(.,AKRO_BBSRI_Dlvry,by="DELIVERY_ID")%>%
  filter(SEL_TO_GENOTYPE == 1)%>%
  mutate(SuccssGTd = ifelse(`WHAT_CARD-NO`%in%Mixtures$indiv,1,0))%>%
  group_by(PLANT_NAME,SW.x,TARGET_SAMPLE_RATE) %>%
  mutate(TARGET_SAMPLE_RATE = round(1/TARGET_SAMPLE_RATE,3))%>%
  summarize(nPrcsd = n(),
            nGenod = sum(SuccssGTd,na.rm=T))%>%
  rename('SW' = 'SW.x')

AKRO_BBSRI_DlvrySum <- AKRO_BBSRI_Dlvry %>%
  group_by(PLANT_NAME,SW,TARGET_SAMPLE_RATE) %>%
  mutate(TARGET_SAMPLE_RATE = round(1/TARGET_SAMPLE_RATE,3))%>%
  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
            nChumSamp = sum(CHUM_SAMPLED,na.rm=T),
            RlzdSampRt = round(nChumSamp / nChum,3))%>%
  left_join(ProcessRate)%>%
  mutate(GenoRate = round(nGenod/nChum,3))%>%
bind_rows(.,data.frame(PLANT_NAME = "Total",
                       nChum = sum(AKRO_BBSRI_Dlvry$CHUM_RETENTION_COUNT,na.rm=T),
                       nChumSamp = sum(AKRO_BBSRI_Dlvry$CHUM_SAMPLED,na.rm=T),
                       RlzdSampRt = round(sum(AKRO_BBSRI_Dlvry$CHUM_SAMPLED,na.rm=T)/sum(AKRO_BBSRI_Dlvry$CHUM_RETENTION_COUNT,na.rm=T),2),
                       nPrcsd = sum(ProcessRate$nPrcsd,na.rm=T),
                       nGenod = sum(ProcessRate$nGenod,na.rm=T),
                       GenoRate = round(sum(ProcessRate$nGenod,na.rm=T)/sum(AKRO_BBSRI_Dlvry$CHUM_RETENTION_COUNT,na.rm=T),3)
                       ))%>%
  filter(nChum>0)%>%ungroup()
```

```{r SampGenoTable, eval=T, echo=F}
options(knitr.kable.NA = '')

# AKRO_BBSRI_DlvrySum$GenoRate[-length(AKRO_BBSRI_DlvrySum)] %>%
#   {.[!is.na(.)]} %>%
#   dist(.) %>%
#   any(.>0.1)
  
AKRO_BBSRI_DlvrySum %>%
  select(-SW) %>%
  knitr::kable(col.names=c("Plant","Target Sample Rate","Total Chum","Chum Sampled","Sample Rate","Chum Genotyped","Chum Analyzed","Genotype Rate"), caption = paste("Chum salmon bycatch sampling and genotyping information for statistical week " , CrntSW, " for the shoreside processing plants. Sampling is grouped by processing plant and target sampling rate. Chum genotyped is the number of chum that were amplified for the marker panel. Chum analyzed are those chum that were genotyped for at least 80\\% of the genetic markers after potential duplicate samples were removed.",sep=""),format = "latex",booktabs=T, linesep = "")%>%
  kableExtra::kable_styling(latex_options = "HOLD_position")%>%
 kableExtra::row_spec(nrow(AKRO_BBSRI_Dlvry %>%
                  group_by(PLANT_NAME,SW,TARGET_SAMPLE_RATE) %>%
                  mutate(TARGET_SAMPLE_RATE = round(1/TARGET_SAMPLE_RATE,3))%>%
                  summarize(nChum = sum(CHUM_RETENTION_COUNT,na.rm=T),
                            nChumSamp = sum(CHUM_SAMPLED,na.rm=T),
                            RlzdSampRt = round(nChumSamp / nChum,3))),hline_after=T)%>%
  kableExtra::column_spec(column = 2:8, width = "0.5in")
```

```{r SetupRuns, eval=T, echo=F}
#starting proportions for the chains
### Starting proportions of each of the chains is 95% of 1 reporting group
  
  Baseline_Reps<-Baseline %>%
    group_by(repunit) %>%
    select(repunit,collection) %>%
    unique()
  
  pi_init_List<-lapply(1:length(unique(Baseline_Reps$repunit)),function(x)
    Baseline_Reps %>%
      mutate(RepPi = ifelse(repunit==unique(Baseline_Reps$repunit)[x],1,2))%>%
      mutate(pi_init = ifelse(RepPi==1,
                              0.95/(Baseline_Reps %>%
                                          filter(repunit==unique(Baseline_Reps$repunit)[x]) %>%
                                          nrow()),
                              0.05/(Baseline_Reps %>%
                                          filter(repunit!=unique(Baseline_Reps$repunit)[x]) %>%
                                          nrow())) ) %>%
      ungroup() %>%
      select(collection,pi_init))
  
### Prior on stock proportions GCg
# A 1/k prior on baseline populations puts a larger probability on 
# reporting groups with many populations in the baseline. So we use a 
# 1/GCg prior to have a more uniform prior over the reporting groups 

RepColls<-unique(Baseline[,c(2,3)])

G<-length(unique(RepColls$repunit))
PriorRep<-RepColls%>%
  group_by(repunit)%>%
  summarise(nPops=length(unique(collection)))%>%
  mutate(GCg=nPops*G)%>%
  mutate(pi=1/GCg)

prior_GCg<-RepColls%>%
  mutate(pi_param=as.numeric(plyr::mapvalues(x=repunit,
                                  from=PriorRep$repunit,
                                  to=PriorRep$pi,)))%>%
  select(collection,pi_param)

AnalyNames <- Mixtures$collection %>% unlist() %>% unique()
```


```{r BSmodelClean, eval=T, echo=F}
GR_Mat<-lapply(1:length(AnalyNames),function(x)
  read_csv(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_NonBootstrapModel/",AnalyNames[[x]],"_Results.csv",sep=""))))%>%
  do.call(rbind,.)%>%
  select(Analysis,repunit,`Point est.`, `Upper C.I.`)%>%
  rename('Region' = "repunit")


BSModRes<-lapply(1:length(AnalyNames),function(x)
  read_csv(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","Rubias_BootstrapModel/",AnalyNames[[x]],"BSModRes.csv",sep=""))))%>%
  do.call(rbind,.)%>%
  select(mixture_collection,repunit,bs_corrected_repunit_ppn,median_BS,sd_BS,loCI_BS,hiCI_BS,P0)%>%
  rename('Analysis' = "mixture_collection",
         'Region' = "repunit",
         'Mean'="bs_corrected_repunit_ppn",
         'Median' = "median_BS",
         'SD' = "sd_BS",
         '2.5% CI' = "loCI_BS",
         '97.5% CI' = "hiCI_BS",
         'P=0' = "P0")

#Now combine all the results for a table
FullRes <- full_join(BSModRes,GR_Mat,by=c("Analysis","Region"))%>%
   rename('SF' = "Point est.")%>%
   select(-'Upper C.I.')%>%
   mutate(Region = recode(Region,
                        `EAsia`="SE Asia",
                        `NAsia`="NE Asia",
                        `WAlaska` = "W Alaska",
                        `SW_Alaska` = "SW Alaska",
                        `UpMidYukon` = "Up/Mid Yukon",
                        `E_GOP_PNW` = "E GOA/PNW",
                        `KotzebueSound` = "Kotzebue Sound"))%>%
   relocate(SD,.after=Mean)%>%
   relocate(Median,.after='2.5% CI')

MixSize<-Mixtures %>% 
   group_by(collection) %>% 
   summarize(MixSize=n())

PSC_num <- data.frame(Analysis = unlist(AnalyNames))%>%
                        left_join(., (AKRO_BBSRI_Dlvry %>%
                                      filter(SW %in% gsub(pattern="SW",replacement = "",unlist(AnalyNames)))%>%
                                      group_by(SW)%>%
                                      summarise(PSC = sum(CHUM_RETENTION_COUNT,na.rm=T))%>%
                                      mutate(Analysis = paste('SW',SW,sep=""))
                        ))%>%
  left_join(.,Mixtures %>% group_by(collection) %>% summarize(Genetic = n()) %>% rename(Analysis = collection))%>%
  mutate(LabelTable = paste("Stat Week ",SW,sep=""))

for (an in 1:length(AnalyNames)){
AnlyName<-AnalyNames%>%
   unlist()%>%
   . [an]
 
PSCtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(PSC)%>%
   unlist() %>%
  as.numeric()

Mixtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(Genetic)%>%
   unlist()

Labtemp<-PSC_num%>%
   filter(Analysis == AnlyName)%>%
   select(LabelTable)%>%
   unlist()
   
PrtyNm3<-function(x) formatC(x,digits=3,format="f",drop0trailing = F)
PrtyNm2<-function(x) formatC(x,digits=2,format="f",drop0trailing = F)


Res_temp<-FullRes %>%
   filter(Analysis == AnlyName)%>%
   mutate(EstNum = round(PSCtemp*Mean),
          EstLo = round(PSCtemp*`2.5% CI`),
          EstHi = round(PSCtemp*`97.5% CI`))%>%
  mutate(EstCI = paste(formatC(round(PSCtemp*`2.5% CI`),big.mark=",",format="d"),
                        formatC(round(PSCtemp*`97.5% CI`),big.mark = ",",format="d"),sep="-"))%>%
   relocate(EstNum,.after=Region)%>%
      relocate(EstLo,.after=EstNum)%>%
   relocate(EstHi,.after=EstLo)%>%
      select(-c(SD,Median))%>%
   mutate(Region_f = factor(Region,
                    levels=c("SE Asia","NE Asia","Kotzebue Sound","W Alaska","Upper/Mid Yukon","SW Alaska","E GOA/PNW")))%>%
   arrange(Region_f) %>%
  select(-Region_f) %>%
  mutate(Region = recode(Region,
                         "SE_Asia" = 'SE Asia' ,
                         "NE_Asia" = 'NE Asia' ,
                         "CWAK" = 'W Alaska',
                         "UpperYukon" = 'Up/Mid Yukon',
                         "KotzebueSound" = 'Kotzebue Sound',
                         "SWAK" = 'SW Alaska',
                         "GOA/PNW" = 'E GOA/PNW' ))
   
TabTitleNumb<-paste("Table 2: Chum salmon bycatch from ",gsub(pattern="Stat Week","statistical week",Labtemp)," of the BSAI trawl fishery (PSC = ",formatC(PSCtemp,big.mark=",",format="d"),
                "; ",
                paste("n = ",formatC(Mixtemp,big.mark=",",format="d") 
                      ,sep=""),
                ")",
          sep="")

TabTitle<-paste(Labtemp,
                paste(" (PSC = ",formatC(PSCtemp,big.mark=",",format="d"),sep=""),
                "; ",
                paste("n = ",formatC(Mixtemp,big.mark=",",format="d") 
                      ,sep=""),
                ")",
          sep="")

Res_temp %>%
  mutate(Region_f = factor(Region,
                    levels=c("SE Asia","NE Asia","Kotzebue Sound","W Alaska","Up/Mid Yukon","SW Alaska","E GOA/PNW")))%>%
   arrange(Region_f) %>%
  select(-Region_f) %>%
  relocate(EstCI,.after=EstNum)%>%
   select(-c(Analysis,EstLo,EstHi))%>%
   mutate(EstNum = formatC(EstNum,big.mark=",",format="d"))%>%
   rename('Est. num.' = "EstNum",
          'Est. CI'="EstCI")%>%
   rename('97.5%' = "97.5% CI")%>%
   rename('2.5%' = "2.5% CI")%>%
   mutate_at(c("Mean","2.5%","97.5%"),PrtyNm3)%>%
   mutate_at(c("P=0","SF"),PrtyNm2)%>%
knitr::kable(.,digits=c(0,0,3,3,3,3,3,3,2),caption=TabTitleNumb,"html",align=c("l","r",rep("c",times=7))) %>%
  kableExtra::kable_classic(full_width = F,html_font = "Times New Roman")%>%
   kableExtra::column_spec(7,background=ifelse(Res_temp$`P=0`>=0.5,"lightgrey","white"))%>%
   kableExtra::save_kable(paste("./Output/",CrntYr,"/",CrntSW,"/",AnlyName,"_Table_Numb.png",sep=""),zoom=10)

Res_temp %>%
  mutate(Region_f = factor(Region,
                    levels=c("SE_Asia","NE_Asia","KotzebueSound","CWAK","Up/Mid Yukon","SWAK","GOA/PNW")))%>%
   arrange(Region_f) %>%
  select(-Region_f) %>%
   select(-c(Analysis,EstLo,EstHi))%>%
   mutate(EstNum = formatC(EstNum,big.mark=",",format="d"))%>%
   rename('Est. num.' = "EstNum",
          'Est. CI'="EstCI")%>%
  relocate('Est. CI',.after='Est. num.')%>%
   rename('97.5%' = "97.5% CI")%>%
   rename('2.5%' = "2.5% CI")%>%
   mutate_at(c("Mean","2.5%","97.5%"),PrtyNm3)%>%
   mutate_at(c("P=0","SF"),PrtyNm2)%>%
knitr::kable(.,digits=c(0,0,3,3,3,3,3,3,2),caption=TabTitle,"html",align=c("l","r",rep("c",times=7))) %>%
  kableExtra::kable_classic(full_width = F,html_font = "Times New Roman")%>%
   kableExtra::column_spec(7,background=ifelse(Res_temp$`P=0`>=0.5,"lightgrey","white"))%>%
   kableExtra::save_kable(paste("./Output/",CrntYr,"/",CrntSW,"/",AnlyName,"_TableAltVrs.png",sep=""),zoom=10)
 
}

write_delim(file = file.path(paste("./Output/",CrntYr,"/",CrntSW,"/GSIresults_SW",CrntSW,".txt",sep="")),
          x= Res_temp,
          delim="\t")
```

## Stock Specific Catches

In statistical week `r CrntSW`, `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% select(EstNum) %>% round(.,0) %>% {.[.>0]} %>% length()` of the seven genetic groups were present in the 
bycatch. The `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% slice_max(Mean) %>% select(Region) %>% unlist()` reporting group comprised the largest proportion of the chum salmon bycatch (`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% slice_max(Mean) %>% select(Mean) %>% unlist() %>% {.*100} %>% round(.,1)`%), 
`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% slice_max(EstNum) %>% select(EstNum) %>% unlist() %>% round(.,0)` of the 
total bycatch of `r AKRO_BBSRI_Dlvry %>% filter(SW %in% gsub(pattern="SW",replacement = "",unlist(AnalyNames)))%>% group_by(SW)%>%summarise(PSC = sum(CHUM_RETENTION_COUNT,na.rm=T))%>%filter(SW==CrntSW)%>%select(PSC)%>% unlist()` 
chum salmon.
The second largest contributing regional group to the bycatch was 
`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% arrange(desc(Mean)) %>% select(Region) %>% unlist() %>% {.[2]}`
with `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% arrange(desc(Mean)) %>% select(Mean) %>% unlist()%>%{.[2]} %>% {.*100} %>% round(.,1)`% or `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% arrange(desc(EstNum)) %>% select(EstNum) %>% unlist() %>% {.[2]} %>% round(.,0)` 
fish. `r if(any(Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% select(EstNum) %>% round(.,0) %>% {.==0})){paste("No chum salmon were caught from the ", Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% mutate(EstNum = round(EstNum,0)) %>% filter(EstNum == 0) %>% select(Region) %>% unlist() %>% paste(.,collapse=", ") %>% {sub(",([^,]*)$", " &\\1", .)},ifelse(Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% mutate(EstNum = round(EstNum,0)) %>% filter(EstNum == 0) %>% select(Region) %>% unlist() %>% length(.) %>% {.==1}," reporting group."," reporting groups"),sep="")}` Western Alaska (Coastal Western Alaska, Kotzebue Sound, 
and Upper/Middle Yukon combined) comprised `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% filter(grepl(pattern = "W Alaska|Kotzebue|Yukon", Region)) %>% select(Mean) %>% sum(.,na.rm=T) %>% unlist() %>% {.*100} %>% round(.,1)`% 
of the bycatch (`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% filter(grepl(pattern = "W Alaska|Kotzebue|Yukon", Region)) %>% select(EstNum) %>% sum(.,na.rm=T) %>% unlist() %>% round(.,0)` fish). 
Asia (NE Asia and SE Asia combined) comprised `r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% filter(grepl(pattern = "Asia", Region)) %>% select(Mean) %>% sum(.,na.rm=T) %>% unlist() %>% {.*100} %>% round(.,1)`% 
of the bycatch (`r Res_temp %>% filter(Analysis == paste("SW",CrntSW,sep="")) %>% filter(grepl(pattern = "Asia", Region)) %>% select(EstNum) %>% sum(.,na.rm=T) %>% unlist() %>% round(.,0)` fish).


```{r SWresTab, echo = FALSE, message=FALSE,out.width='5in',fig.align = 'center'}
knitr::include_graphics(file.path(paste("./Output/",CrntYr,"/",CrntSW,"/","SW",CrntSW,"_Table_Numb.png",sep="")))
```

```{r PriorResChk, eval=T, echo=F}
GsiRes2Read <- list.files(file.path(paste("./Output/",CrntYr,"/",sep="")),recursive=T,full.names = T)%>%
  grep(pattern="GSIresults_",value=T)

GSIResSW <- lapply(str_split(GsiRes2Read,pattern="_|\\."),"[[",3)%>%
  unlist()%>%
  gsub(pattern="SW",replacement="",.)%>%
  as.numeric()

GsiRes2Read<- data.frame(File = GsiRes2Read,
                         SW = GSIResSW) %>%
  filter(SW <= CrntSW)%>%
  select(File)%>%
  unlist()

PriorRes <- ifelse(length(GsiRes2Read)>1,"TRUE","FALSE")
```


```{r PriorResGraph, eval=PriorRes, echo=F, fig.height=6,fig.width=7, fig.cap="Figure 3: Chum salmon bycatch proportion (A) and total number of fish (B) for the B season of the pollock trawl fishery."}
GsiRes2Read <- list.files(file.path(paste("./Output/",CrntYr,"/",sep="")),recursive=T,full.names = T)%>%
  grep(pattern="GSIresults_",value=T)

GSIResSW <- lapply(str_split(GsiRes2Read,pattern="_|\\."),"[[",3)%>%
  unlist()%>%
  gsub(pattern="SW",replacement="",.)%>%
  as.numeric()

GsiRes2Read<- data.frame(File = GsiRes2Read,
                         SW = GSIResSW) %>%
  filter(SW <= CrntSW)%>%
  select(File)%>%
  unlist()

  GSI_ResSWs <- lapply(1:length(GsiRes2Read),function(x) read_delim(GsiRes2Read[x],delim="\t")) %>%
                         do.call(rbind,.)

pProp <- GSI_ResSWs %>%
  mutate(SW = gsub(pattern="SW",replacement="",Analysis))%>%
  select(SW,Region,Mean,`2.5% CI`,`97.5% CI`)%>%
  mutate(Region_f = factor(Region,levels = c("SE Asia","NE Asia","Kotzebue Sound","W Alaska","Up/Mid Yukon","SW Alaska","E GOA/PNW")))%>%
  ggplot(.,aes(x=SW,y=Mean,color=Region_f,group=Region_f))+
    geom_point()+
    geom_line()+
  geom_errorbar(aes(ymin=`2.5% CI`, ymax=`97.5% CI`), width=.1)+
    theme_bw()+
    ylab('Stock Proportion')+
    xlab('Statistical Week')+
  theme(legend.position = "top")+
  guides(color=guide_legend(title="Genetic\nGroup"))
  
  
pNumb <- GSI_ResSWs %>%
  mutate(SW = gsub(pattern="SW",replacement="",Analysis))%>%
  select(SW,Region,EstNum,EstLo,EstHi)%>%
  mutate(Region_f = factor(Region,levels = c("SE Asia","NE Asia","Kotzebue Sound","W Alaska","Up/Mid Yukon","SW Alaska","E GOA/PNW")))%>%
  ggplot(.,aes(x=SW,y=EstNum,color=Region_f,group=Region_f))+
    geom_point()+
    geom_line()+
  geom_errorbar(aes(ymin=EstLo, ymax=EstHi), width=.1)+
    theme_bw()+
    ylab('Number Chum Salmon')+
    xlab('Statistical Week')+
  scale_y_continuous(label=scales::comma)+
  theme(legend.position = "none")
  
cowplot::plot_grid(pProp,
                   pNumb,
                   labels = c("A","B"),
                   rel_heights = c(0.99,1),
                   nrow=2) 
  
```


`r ifelse(PrntByctchSW==T, paste("Of the ", AKRO_Obs$CHUM_RETENTION_COUNT %>% sum(.,na.rm=T) %>% formatC(.,big.mark=",",format="d"), " chum salmon caught by the shoreside sector since statistical week ", min(AKRO_ObsSum$SW,na.rm=T), ", the largest contributing genetic group to the bycatch has been the ", GSI_ResSWs %>% group_by(Region) %>% summarize(nChum = sum(EstNum,na.rm=T)) %>% slice_max(nChum) %>% select(Region) %>% unlist(),", with a point estimate of ", GSI_ResSWs %>% group_by(Region) %>% summarize(nChum = sum(EstNum,na.rm=T)) %>% slice_max(nChum) %>% select(nChum) %>% unlist() %>% formatC(.,big.mark=",",format="f",digits=0)," chum salmon."  ,sep=""))` `r ifelse(PrntByctchSW==T, paste("Since the start of the B season, the Western Alaska group has comprised an average of ", GSI_ResSWs %>% filter(Region == "W Alaska") %>% select(Mean) %>% unlist() %>% mean(.) %>% round(.,3) %>% {.*100} ,"% of the bycatch, with the shoreside sector having caught a total of ", GSI_ResSWs %>% filter(Region == "W Alaska") %>% select(EstNum) %>% unlist() %>% sum(.,na.rm=T) %>% round(.,0) %>% formatC(.,big.mark=",",format="d"), " chum salmon from this genetic group. The Kotzebue Sound regional group made up an average of ", GSI_ResSWs %>% filter(Region == "Kotzebue Sound") %>% select(Mean) %>% unlist() %>% mean(.) %>% round(.,3) %>% {.*100},"% of the bycatch over the ",length(GSIResSW)," weeks, with a total of ",GSI_ResSWs %>% filter(Region == "Kotzebue Sound") %>% select(EstNum) %>% unlist() %>% sum(.,na.rm=T) %>% round(.,0) %>% formatC(.,big.mark=",",format="d"), " chum salmon harvested to date. The Upper/Middle Yukon group accounted for an average of ", GSI_ResSWs %>% filter(Region == "Up/Mid Yukon") %>% select(Mean) %>% unlist() %>% mean(.) %>% round(.,3) %>% {.*100},"% of the chum salmon bycatch, with ", GSI_ResSWs %>% filter(Region == "Up/Mid Yukon") %>% select(EstNum) %>% unlist() %>% sum(.,na.rm=T) %>% round(.,0) %>% formatC(.,big.mark=",",format="d"), " fish harvested since statistical week ",min(GSIResSW),sep=""),"")`. 


\newpage
# References
<div id="refs"></div>

\newpage

\blandscape
# Appendix I - Genetic loci information
```{r PP1, echo = FALSE, message=FALSE,out.height='6.5in',out.width='9in',fig.align = 'left'}
#dt("PP1", "Primer Probe file for chum salmon GTseq panel.")
knitr::include_graphics("./MainData/LocusSummaryTab_1-42.png")
```
\newpage
```{r PP2, echo = FALSE, message=FALSE,out.height='6.5in',out.width='9in',fig.align = 'left'}
#dt("PP2", "Continued: Primer Probe file for chum salmon GTseq panel.")
knitr::include_graphics("./MainData/LocusSummaryTab_43-84.png")
```
\elandscape

\newpage

```{r APIIprntDcsn, eval=T, echo=F}
#Get the number of results files
files <- list.files(path = file.path(paste("./Output/",CrntYr,sep="")), recursive = T,pattern = "png", full.names = TRUE)%>%
     grep(pattern="AltVrs",x=.,invert=F,value=T) %>%
     grep(pattern=CrntSW,invert = T,value=T)

#if the number of analysese is > 0, then print all the old results.
prntAII <- ifelse(length(files)>0,TRUE,FALSE)
```


```{r conditional_print, child='AppendixII.Rmd', eval = prntAII}
```

\newpage


